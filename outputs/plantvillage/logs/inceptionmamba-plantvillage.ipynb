{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# PlantXMamba/mamba_block/pscan.py\nimport math\n\nimport torch\nimport torch.nn.functional as F\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport os\nfrom tqdm import tqdm\nimport torchvision.models as models\nfrom torchvision.models import VGG16_Weights\n\n\ndef npo2(len):\n    \"\"\"\n    Returns the next power of 2 above len\n    \"\"\"\n\n    return 2 ** math.ceil(math.log2(len))\n\n\ndef pad_npo2(X):\n    \"\"\"\n    Pads input length dim to the next power of 2\n\n    Args:\n        X : (B, L, D, N)\n\n    Returns:\n        Y : (B, npo2(L), D, N)\n    \"\"\"\n\n    len_npo2 = npo2(X.size(1))\n    pad_tuple = (0, 0, 0, 0, 0, len_npo2 - X.size(1))\n    return F.pad(X, pad_tuple, \"constant\", 0)\n\n\nclass PScan(torch.autograd.Function):\n    @staticmethod\n    def pscan(A, X):\n        # A : (B, D, L, N)\n        # X : (B, D, L, N)\n\n        # modifies X in place by doing a parallel scan.\n        # more formally, X will be populated by these values :\n        # H[t] = A[t] * H[t-1] + X[t] with H[0] = 0\n        # which are computed in parallel (2*log2(T) sequential steps (ideally), instead of T sequential steps)\n\n        # only supports L that is a power of two (mainly for a clearer code)\n\n        B, D, L, _ = A.size()\n        num_steps = int(math.log2(L))\n\n        # up sweep (last 2 steps unfolded)\n        Aa = A\n        Xa = X\n        for _ in range(num_steps - 2):\n            T = Xa.size(2)\n            Aa = Aa.view(B, D, T // 2, 2, -1)\n            Xa = Xa.view(B, D, T // 2, 2, -1)\n\n            Xa[:, :, :, 1].add_(Aa[:, :, :, 1].mul(Xa[:, :, :, 0]))\n            Aa[:, :, :, 1].mul_(Aa[:, :, :, 0])\n\n            Aa = Aa[:, :, :, 1]\n            Xa = Xa[:, :, :, 1]\n\n        # we have only 4, 2 or 1 nodes left\n        if Xa.size(2) == 4:\n            Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 0]))\n            Aa[:, :, 1].mul_(Aa[:, :, 0])\n\n            Xa[:, :, 3].add_(\n                Aa[:, :, 3].mul(Xa[:, :, 2] + Aa[:, :, 2].mul(Xa[:, :, 1]))\n            )\n        elif Xa.size(2) == 2:\n            Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 0]))\n            return\n        else:\n            return\n\n        # down sweep (first 2 steps unfolded)\n        Aa = A[:, :, 2 ** (num_steps - 2) - 1 : L : 2 ** (num_steps - 2)]\n        Xa = X[:, :, 2 ** (num_steps - 2) - 1 : L : 2 ** (num_steps - 2)]\n        Xa[:, :, 2].add_(Aa[:, :, 2].mul(Xa[:, :, 1]))\n        Aa[:, :, 2].mul_(Aa[:, :, 1])\n\n        for k in range(num_steps - 3, -1, -1):\n            Aa = A[:, :, 2**k - 1 : L : 2**k]\n            Xa = X[:, :, 2**k - 1 : L : 2**k]\n\n            T = Xa.size(2)\n            Aa = Aa.view(B, D, T // 2, 2, -1)\n            Xa = Xa.view(B, D, T // 2, 2, -1)\n\n            Xa[:, :, 1:, 0].add_(Aa[:, :, 1:, 0].mul(Xa[:, :, :-1, 1]))\n            Aa[:, :, 1:, 0].mul_(Aa[:, :, :-1, 1])\n\n    @staticmethod\n    def pscan_rev(A, X):\n        # A : (B, D, L, N)\n        # X : (B, D, L, N)\n\n        # the same function as above, but in reverse\n        # (if you flip the input, call pscan, then flip the output, you get what this function outputs)\n        # it is used in the backward pass\n\n        # only supports L that is a power of two (mainly for a clearer code)\n\n        B, D, L, _ = A.size()\n        num_steps = int(math.log2(L))\n\n        # up sweep (last 2 steps unfolded)\n        Aa = A\n        Xa = X\n        for _ in range(num_steps - 2):\n            T = Xa.size(2)\n            Aa = Aa.view(B, D, T // 2, 2, -1)\n            Xa = Xa.view(B, D, T // 2, 2, -1)\n\n            Xa[:, :, :, 0].add_(Aa[:, :, :, 0].mul(Xa[:, :, :, 1]))\n            Aa[:, :, :, 0].mul_(Aa[:, :, :, 1])\n\n            Aa = Aa[:, :, :, 0]\n            Xa = Xa[:, :, :, 0]\n\n        # we have only 4, 2 or 1 nodes left\n        if Xa.size(2) == 4:\n            Xa[:, :, 2].add_(Aa[:, :, 2].mul(Xa[:, :, 3]))\n            Aa[:, :, 2].mul_(Aa[:, :, 3])\n\n            Xa[:, :, 0].add_(\n                Aa[:, :, 0].mul(Xa[:, :, 1].add(Aa[:, :, 1].mul(Xa[:, :, 2])))\n            )\n        elif Xa.size(2) == 2:\n            Xa[:, :, 0].add_(Aa[:, :, 0].mul(Xa[:, :, 1]))\n            return\n        else:\n            return\n\n        # down sweep (first 2 steps unfolded)\n        Aa = A[:, :, 0 : L : 2 ** (num_steps - 2)]\n        Xa = X[:, :, 0 : L : 2 ** (num_steps - 2)]\n        Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 2]))\n        Aa[:, :, 1].mul_(Aa[:, :, 2])\n\n        for k in range(num_steps - 3, -1, -1):\n            Aa = A[:, :, 0 : L : 2**k]\n            Xa = X[:, :, 0 : L : 2**k]\n\n            T = Xa.size(2)\n            Aa = Aa.view(B, D, T // 2, 2, -1)\n            Xa = Xa.view(B, D, T // 2, 2, -1)\n\n            Xa[:, :, :-1, 1].add_(Aa[:, :, :-1, 1].mul(Xa[:, :, 1:, 0]))\n            Aa[:, :, :-1, 1].mul_(Aa[:, :, 1:, 0])\n\n    @staticmethod\n    def forward(ctx, A_in, X_in):\n        \"\"\"\n        Applies the parallel scan operation, as defined above. Returns a new tensor.\n        If you can, privilege sequence lengths that are powers of two.\n\n        Args:\n            A_in : (B, L, D, N)\n            X_in : (B, L, D, N)\n\n        Returns:\n            H : (B, L, D, N)\n        \"\"\"\n\n        L = X_in.size(1)\n\n        # cloning is requiered because of the in-place ops\n        if L == npo2(L):\n            A = A_in.clone()\n            X = X_in.clone()\n        else:\n            # pad tensors (and clone btw)\n            A = pad_npo2(A_in)  # (B, npo2(L), D, N)\n            X = pad_npo2(X_in)  # (B, npo2(L), D, N)\n\n        # prepare tensors\n        A = A.transpose(2, 1)  # (B, D, npo2(L), N)\n        X = X.transpose(2, 1)  # (B, D, npo2(L), N)\n\n        # parallel scan (modifies X in-place)\n        PScan.pscan(A, X)\n\n        ctx.save_for_backward(A_in, X)\n\n        # slice [:, :L] (cut if there was padding)\n        return X.transpose(2, 1)[:, :L]\n\n    @staticmethod\n    def backward(ctx, grad_output_in):\n        \"\"\"\n        Flows the gradient from the output to the input. Returns two new tensors.\n\n        Args:\n            ctx : A_in : (B, L, D, N), X : (B, D, L, N)\n            grad_output_in : (B, L, D, N)\n\n        Returns:\n            gradA : (B, L, D, N), gradX : (B, L, D, N)\n        \"\"\"\n\n        A_in, X = ctx.saved_tensors\n\n        L = grad_output_in.size(1)\n\n        # cloning is requiered because of the in-place ops\n        if L == npo2(L):\n            grad_output = grad_output_in.clone()\n            # the next padding will clone A_in\n        else:\n            grad_output = pad_npo2(grad_output_in)  # (B, npo2(L), D, N)\n            A_in = pad_npo2(A_in)  # (B, npo2(L), D, N)\n\n        # prepare tensors\n        grad_output = grad_output.transpose(2, 1)\n        A_in = A_in.transpose(2, 1)  # (B, D, npo2(L), N)\n        A = torch.nn.functional.pad(\n            A_in[:, :, 1:], (0, 0, 0, 1)\n        )  # (B, D, npo2(L), N) shift 1 to the left (see hand derivation)\n\n        # reverse parallel scan (modifies grad_output in-place)\n        PScan.pscan_rev(A, grad_output)\n\n        Q = torch.zeros_like(X)\n        Q[:, :, 1:].add_(X[:, :, :-1] * grad_output[:, :, 1:])\n\n        return Q.transpose(2, 1)[:, :L], grad_output.transpose(2, 1)[:, :L]\n\n\npscan = PScan.apply\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T06:59:42.658518Z","iopub.execute_input":"2025-06-29T06:59:42.658776Z","iopub.status.idle":"2025-06-29T06:59:42.680994Z","shell.execute_reply.started":"2025-06-29T06:59:42.658753Z","shell.execute_reply":"2025-06-29T06:59:42.680392Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# PlantXMamba/mamba_block/backbone.py\nimport math\nfrom dataclasses import dataclass\nfrom typing import Union\n\n\n\"\"\"\n\nThis file closely follows the mamba_simple.py from the official Mamba implementation, and the mamba-minimal by @johnma2006.\nThe major differences are :\n-the convolution is done with torch.nn.Conv1d\n-the selective scan is done in PyTorch\n\nA sequential version of the selective scan is also available for comparison. Also, it is possible to use the official Mamba implementation.\n\nThis is the structure of the torch modules :\n- A Mamba model is composed of several layers, which are ResidualBlock.\n- A ResidualBlock is composed of a MambaBlock, a normalization, and a residual connection : ResidualBlock(x) = mamba(norm(x)) + x\n- This leaves us with the MambaBlock : its input x is (B, L, D) and its outputs y is also (B, L, D) (B=batch size, L=seq len, D=model dim).\nFirst, we expand x into (B, L, 2*ED) (where E is usually 2) and split it into x and z, each (B, L, ED).\nThen, we apply the short 1d conv to x, followed by an activation function (silu), then the SSM.\nWe then multiply it by silu(z).\nSee Figure 3 of the paper (page 8) for a visual representation of a MambaBlock.\n\n\"\"\"\n\n\n@dataclass\nclass MambaConfig:\n    d_model: int  # D\n    n_layers: int\n    dt_rank: Union[int, str] = \"auto\"\n    d_state: int = 16  # N in paper/comments\n    expand_factor: int = 2  # E in paper/comments\n    d_conv: int = 4\n\n    dt_min: float = 0.001\n    dt_max: float = 0.1\n    dt_init: str = \"random\"  # \"random\" or \"constant\"\n    dt_scale: float = 1.0\n    dt_init_floor = 1e-4\n\n    rms_norm_eps: float = 1e-5\n    base_std: float = 0.02\n\n    dropout: float = 0.1\n\n    bias: bool = False\n    conv_bias: bool = True\n    inner_layernorms: bool = False  # apply layernorms to internal activations\n\n    mup: bool = False\n    mup_base_width: float = 128  # width=d_model\n\n    pscan: bool = True  # use parallel scan mode or sequential mode when training\n    use_cuda: bool = False  # use official CUDA implementation when training (not compatible with (b)float16)\n\n    def __post_init__(self):\n        self.d_inner = self.expand_factor * self.d_model  # E*D = ED in comments\n\n        if self.dt_rank == \"auto\":\n            self.dt_rank = math.ceil(self.d_model / 16)\n\n        # muP\n        if self.mup:\n            self.mup_width_mult = self.d_model / self.mup_base_width\n\n\nclass Mamba(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n\n        self.config = config\n\n        self.layers = nn.ModuleList(\n            [ResidualBlock(config) for _ in range(config.n_layers)]\n        )\n\n    def forward(self, x):\n        # x : (B, L, D)\n\n        # y : (B, L, D)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        return x\n\n    def step(self, x, caches):\n        # x : (B, L, D)\n        # caches : [cache(layer) for all layers], cache : (h, inputs)\n\n        # y : (B, L, D)\n        # caches : [cache(layer) for all layers], cache : (h, inputs)\n\n        for i, layer in enumerate(self.layers):\n            x, caches[i] = layer.step(x, caches[i])\n\n        return x, caches\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n\n        self.mixer = MambaBlock(config)\n        self.norm = RMSNorm(config.d_model, config.rms_norm_eps, config.mup)\n\n    def forward(self, x):\n        # x : (B, L, D)\n\n        # output : (B, L, D)\n\n        output = self.mixer(self.norm(x)) + x\n        return output\n\n    def step(self, x, cache):\n        # x : (B, D)\n        # cache : (h, inputs)\n        # h : (B, ED, N)\n        # inputs: (B, ED, d_conv-1)\n\n        # output : (B, D)\n        # cache : (h, inputs)\n\n        output, cache = self.mixer.step(self.norm(x), cache)\n        output = output + x\n        return output, cache\n\n\nclass MambaBlock(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n\n        self.config = config\n\n        # projects block input from D to 2*ED (two branches)\n        self.in_proj = nn.Linear(config.d_model, 2 * config.d_inner, bias=config.bias)\n\n        self.conv1d = nn.Conv1d(\n            in_channels=config.d_inner,\n            out_channels=config.d_inner,\n            kernel_size=config.d_conv,\n            bias=config.conv_bias,\n            groups=config.d_inner,\n            padding=config.d_conv - 1,\n        )\n\n        # projects x to input-dependent delta, B, C\n        self.x_proj = nn.Linear(\n            config.d_inner, config.dt_rank + 2 * config.d_state, bias=False\n        )\n\n        # projects delta from dt_rank to d_inner\n        self.dt_proj = nn.Linear(config.dt_rank, config.d_inner, bias=True)\n\n        # dt initialization\n        # dt weights\n        dt_init_std = config.dt_rank**-0.5 * config.dt_scale\n        if config.dt_init == \"constant\":\n            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n        elif config.dt_init == \"random\":\n            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n        else:\n            raise NotImplementedError\n\n        # delta bias\n        dt = torch.exp(\n            torch.rand(config.d_inner)\n            * (math.log(config.dt_max) - math.log(config.dt_min))\n            + math.log(config.dt_min)\n        ).clamp(min=config.dt_init_floor)\n        inv_dt = dt + torch.log(\n            -torch.expm1(-dt)\n        )  # inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n        with torch.no_grad():\n            self.dt_proj.bias.copy_(inv_dt)\n        # self.dt_proj.bias._no_reinit = True # initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n        # todo : explain why removed\n\n        # S4D real initialization\n        A = torch.arange(1, config.d_state + 1, dtype=torch.float32).repeat(\n            config.d_inner, 1\n        )\n        self.A_log = nn.Parameter(\n            torch.log(A)\n        )  # why store A in log ? to keep A < 0 (cf -torch.exp(...)) ? for gradient stability ?\n        self.A_log._no_weight_decay = True\n\n        self.D = nn.Parameter(torch.ones(config.d_inner))\n        self.D._no_weight_decay = True\n\n        # projects block output from ED back to D\n        self.out_proj = nn.Linear(config.d_inner, config.d_model, bias=config.bias)\n\n        # used in jamba\n        if self.config.inner_layernorms:\n            self.dt_layernorm = RMSNorm(\n                self.config.dt_rank, config.rms_norm_eps, config.mup\n            )\n            self.B_layernorm = RMSNorm(\n                self.config.d_state, config.rms_norm_eps, config.mup\n            )\n            self.C_layernorm = RMSNorm(\n                self.config.d_state, config.rms_norm_eps, config.mup\n            )\n        else:\n            self.dt_layernorm = None\n            self.B_layernorm = None\n            self.C_layernorm = None\n\n        if self.config.use_cuda:\n            try:\n                from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n\n                self.selective_scan_cuda = selective_scan_fn\n            except ImportError:\n                print(\"Failed to import mamba_ssm. Falling back to mamba.py.\")\n                self.config.use_cuda = False\n\n    def _apply_layernorms(self, dt, B, C):\n        if self.dt_layernorm is not None:\n            dt = self.dt_layernorm(dt)\n        if self.B_layernorm is not None:\n            B = self.B_layernorm(B)\n        if self.C_layernorm is not None:\n            C = self.C_layernorm(C)\n        return dt, B, C\n\n    def forward(self, x):\n        # x : (B, L, D)\n\n        # y : (B, L, D)\n\n        _, L, _ = x.shape\n\n        xz = self.in_proj(x)  # (B, L, 2*ED)\n        x, z = xz.chunk(2, dim=-1)  # (B, L, ED), (B, L, ED)\n\n        # x branch\n        x = x.transpose(1, 2)  # (B, ED, L)\n        x = self.conv1d(x)[\n            :, :, :L\n        ]  # depthwise convolution over time, with a short filter\n        x = x.transpose(1, 2)  # (B, L, ED)\n\n        x = F.silu(x)\n        y = self.ssm(x, z)\n\n        if self.config.use_cuda:\n            output = self.out_proj(y)  # (B, L, D)\n            return output  # the rest of the operations are done in the ssm function (fused with the CUDA pscan)\n\n        # z branch\n        z = F.silu(z)\n\n        output = y * z\n        output = self.out_proj(output)  # (B, L, D)\n\n        return output\n\n    def ssm(self, x, z):\n        # x : (B, L, ED)\n\n        # y : (B, L, ED)\n\n        A = -torch.exp(self.A_log.float())  # (ED, N)\n        D = self.D.float()\n\n        deltaBC = self.x_proj(x)  # (B, L, dt_rank+2*N)\n        delta, B, C = torch.split(\n            deltaBC,\n            [self.config.dt_rank, self.config.d_state, self.config.d_state],\n            dim=-1,\n        )  # (B, L, dt_rank), (B, L, N), (B, L, N)\n        delta, B, C = self._apply_layernorms(delta, B, C)\n        delta = self.dt_proj.weight @ delta.transpose(\n            1, 2\n        )  # (ED, dt_rank) @ (B, L, dt_rank) -> (B, ED, L)\n        # here we just apply the matrix mul operation of delta = softplus(dt_proj(delta))\n        # the rest will be applied later (fused if using cuda)\n\n        # choose which selective_scan function to use, according to config\n        if self.config.use_cuda:\n            # these are unfortunately needed for the selective_scan_cuda function\n            x = x.transpose(1, 2)\n            B = B.transpose(1, 2)\n            C = C.transpose(1, 2)\n            z = z.transpose(1, 2)\n\n            # \"softplus\" + \"bias\" + \"y * silu(z)\" operations are fused\n            y = self.selective_scan_cuda(\n                x,\n                delta,\n                A,\n                B,\n                C,\n                D,\n                z=z,\n                delta_softplus=True,\n                delta_bias=self.dt_proj.bias.float(),\n            )\n            y = y.transpose(1, 2)  # (B, L, ED)\n\n        else:\n            delta = delta.transpose(1, 2)\n            delta = F.softplus(delta + self.dt_proj.bias)\n\n            if self.config.pscan:\n                y = self.selective_scan(x, delta, A, B, C, D)\n            else:\n                y = self.selective_scan_seq(x, delta, A, B, C, D)\n\n        return y\n\n    def selective_scan(self, x, delta, A, B, C, D):\n        # x : (B, L, ED)\n        # Δ : (B, L, ED)\n        # A : (ED, N)\n        # B : (B, L, N)\n        # C : (B, L, N)\n        # D : (ED)\n\n        # y : (B, L, ED)\n\n        deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, L, ED, N)\n        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  # (B, L, ED, N)\n\n        BX = deltaB * (x.unsqueeze(-1))  # (B, L, ED, N)\n\n        hs = pscan(deltaA, BX)\n\n        y = (hs @ C.unsqueeze(-1)).squeeze(\n            3\n        )  # (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n\n        y = y + D * x\n\n        return y\n\n    def selective_scan_seq(self, x, delta, A, B, C, D):\n        # x : (B, L, ED)\n        # Δ : (B, L, ED)\n        # A : (ED, N)\n        # B : (B, L, N)\n        # C : (B, L, N)\n        # D : (ED)\n\n        # y : (B, L, ED)\n\n        _, L, _ = x.shape\n\n        deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, L, ED, N)\n        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  # (B, L, ED, N)\n\n        BX = deltaB * (x.unsqueeze(-1))  # (B, L, ED, N)\n\n        h = torch.zeros(\n            x.size(0), self.config.d_inner, self.config.d_state, device=deltaA.device\n        )  # (B, ED, N)\n        hs = []\n\n        for t in range(0, L):\n            h = deltaA[:, t] * h + BX[:, t]\n            hs.append(h)\n\n        hs = torch.stack(hs, dim=1)  # (B, L, ED, N)\n\n        y = (hs @ C.unsqueeze(-1)).squeeze(\n            3\n        )  # (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n\n        y = y + D * x\n\n        return y\n\n    # -------------------------- inference -------------------------- #\n    \"\"\"\n    Concerning auto-regressive inference\n\n    The cool part of using Mamba : inference is constant wrt to sequence length\n    We just have to keep in cache, for each layer, two things :\n    - the hidden state h (which is (B, ED, N)), as you typically would when doing inference with a RNN\n    - the last d_conv-1 inputs of the layer, to be able to compute the 1D conv which is a convolution over the time dimension\n      (d_conv is fixed so this doesn't incur a growing cache as we progress on generating the sequence)\n      (and d_conv is usually very small, like 4, so we just have to \"remember\" the last 3 inputs)\n\n    Concretely, these two quantities are put inside a cache tuple, and are named h and inputs respectively.\n    h is (B, ED, N), and inputs is (B, ED, d_conv-1)\n    The MambaBlock.step() receives this cache, and, along with outputing the output, alos outputs the updated cache for the next call.\n\n    The cache object is initialized as follows : (None, torch.zeros()).\n    When h is None, the selective scan function detects it and start with h=0.\n    The torch.zeros() isn't a problem (it's same as just feeding the input, because the conv1d is padded)\n\n    As we need one such cache variable per layer, we store a caches object, which is simply a list of cache object. (See mamba_lm.py)\n    \"\"\"\n\n    def step(self, x, cache):\n        # x : (B, D)\n        # cache : (h, inputs)\n        # h : (B, ED, N)\n        # inputs : (B, ED, d_conv-1)\n\n        # y : (B, D)\n        # cache : (h, inputs)\n\n        h, inputs = cache\n\n        xz = self.in_proj(x)  # (B, 2*ED)\n        x, z = xz.chunk(2, dim=1)  # (B, ED), (B, ED)\n\n        # x branch\n        x_cache = x.unsqueeze(2)\n        x = self.conv1d(torch.cat([inputs, x_cache], dim=2))[\n            :, :, self.config.d_conv - 1\n        ]  # (B, ED)\n\n        x = F.silu(x)\n        y, h = self.ssm_step(x, h)\n\n        # z branch\n        z = F.silu(z)\n\n        output = y * z\n        output = self.out_proj(output)  # (B, D)\n\n        # prepare cache for next call\n        inputs = torch.cat([inputs[:, :, 1:], x_cache], dim=2)  # (B, ED, d_conv-1)\n        cache = (h, inputs)\n\n        return output, cache\n\n    def ssm_step(self, x, h):\n        # x : (B, ED)\n        # h : (B, ED, N)\n\n        # y : (B, ED)\n        # h : (B, ED, N)\n\n        A = -torch.exp(\n            self.A_log.float()\n        )  # (ED, N) # todo : ne pas le faire tout le temps, puisque c'est indépendant de la timestep\n        D = self.D.float()\n\n        deltaBC = self.x_proj(x)  # (B, dt_rank+2*N)\n\n        delta, B, C = torch.split(\n            deltaBC,\n            [self.config.dt_rank, self.config.d_state, self.config.d_state],\n            dim=-1,\n        )  # (B, dt_rank), (B, N), (B, N)\n        delta, B, C = self._apply_layernorms(delta, B, C)\n        delta = F.softplus(self.dt_proj(delta))  # (B, ED)\n\n        deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, ED, N)\n        deltaB = delta.unsqueeze(-1) * B.unsqueeze(1)  # (B, ED, N)\n\n        BX = deltaB * (x.unsqueeze(-1))  # (B, ED, N)\n\n        if h is None:\n            h = torch.zeros(\n                x.size(0),\n                self.config.d_inner,\n                self.config.d_state,\n                device=deltaA.device,\n            )  # (B, ED, N)\n\n        h = deltaA * h + BX  # (B, ED, N)\n\n        y = (h @ C.unsqueeze(-1)).squeeze(2)  # (B, ED, N) @ (B, N, 1) -> (B, ED, 1)\n\n        y = y + D * x\n\n        return y, h\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, d_model: int, eps: float = 1e-5, use_mup: bool = False):\n        super().__init__()\n\n        self.use_mup = use_mup\n        self.eps = eps\n\n        # https://arxiv.org/abs/2404.05728, RMSNorm gains prevents muTransfer (section 4.2.3)\n        if not use_mup:\n            self.weight = nn.Parameter(torch.ones(d_model))\n\n    def forward(self, x):\n        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n        if not self.use_mup:\n            return output * self.weight\n        else:\n            return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T06:59:43.279909Z","iopub.execute_input":"2025-06-29T06:59:43.280233Z","iopub.status.idle":"2025-06-29T06:59:43.442098Z","shell.execute_reply.started":"2025-06-29T06:59:43.280211Z","shell.execute_reply":"2025-06-29T06:59:43.441405Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# PlantXMamba/mamba_block/head.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass MambaHead(nn.Module):\n    def __init__(self, d_model: int, dropout: float = 0.0):\n        super().__init__()\n        self.norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.norm(x)\n        x = self.dropout(x)\n        return x  # (batch_size, seq_len, d_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T07:00:07.297816Z","iopub.execute_input":"2025-06-29T07:00:07.298111Z","iopub.status.idle":"2025-06-29T07:00:07.303204Z","shell.execute_reply.started":"2025-06-29T07:00:07.298087Z","shell.execute_reply":"2025-06-29T07:00:07.302443Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# PlantXMamba/mamba_block/model.py\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MambaModule(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        self.d_model = self.args.d_model\n        self.n_layers = self.args.n_layers\n\n        config = MambaConfig(d_model=self.d_model, n_layers=self.n_layers,\n                           d_state=self.args.d_state, d_conv=self.args.d_conv,\n                           expand_factor=self.args.expand,dropout=self.args.dropout)\n        self.backbone = Mamba(config)\n        self.head = MambaHead(d_model=self.d_model, dropout=self.args.dropout)\n\n    def forward(self, x):\n        sequence_output = self.backbone(x)  # (batch_size, seq_len, d_model)\n        output = self.head(sequence_output)  # (batch_size, seq_len, d_model)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T07:00:09.035883Z","iopub.execute_input":"2025-06-29T07:00:09.036210Z","iopub.status.idle":"2025-06-29T07:00:09.041857Z","shell.execute_reply.started":"2025-06-29T07:00:09.036185Z","shell.execute_reply":"2025-06-29T07:00:09.041053Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!git clone https://github.com/sakanaowo/PlantXViT\n!git clone https://github.com/sakanaowo/PlantXMamba","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T07:00:12.197469Z","iopub.execute_input":"2025-06-29T07:00:12.198014Z","iopub.status.idle":"2025-06-29T07:02:37.687758Z","shell.execute_reply.started":"2025-06-29T07:00:12.197987Z","shell.execute_reply":"2025-06-29T07:02:37.687009Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'PlantXViT'...\nremote: Enumerating objects: 104825, done.\u001b[K\nremote: Counting objects: 100% (23/23), done.\u001b[K\nremote: Compressing objects: 100% (17/17), done.\u001b[K\nremote: Total 104825 (delta 6), reused 20 (delta 4), pack-reused 104802 (from 1)\u001b[K\nReceiving objects: 100% (104825/104825), 2.45 GiB | 23.81 MiB/s, done.\nResolving deltas: 100% (30447/30447), done.\nUpdating files: 100% (104353/104353), done.\nCloning into 'PlantXMamba'...\nremote: Enumerating objects: 281, done.\u001b[K\nremote: Counting objects: 100% (13/13), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 281 (delta 1), reused 11 (delta 1), pack-reused 268 (from 1)\u001b[K\nReceiving objects: 100% (281/281), 76.73 MiB | 20.84 MiB/s, done.\nResolving deltas: 100% (124/124), done.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T06:10:16.661964Z","iopub.execute_input":"2025-06-29T06:10:16.662656Z","iopub.status.idle":"2025-06-29T06:10:16.793139Z","shell.execute_reply.started":"2025-06-29T06:10:16.662616Z","shell.execute_reply":"2025-06-29T06:10:16.792214Z"}},"outputs":[{"name":"stdout","text":"PlantXMamba  PlantXViT\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T07:25:08.414726Z","iopub.execute_input":"2025-06-29T07:25:08.415501Z","iopub.status.idle":"2025-06-29T07:25:09.203823Z","shell.execute_reply.started":"2025-06-29T07:25:08.415467Z","shell.execute_reply":"2025-06-29T07:25:09.203280Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models import inception_v3, Inception_V3_Weights\n\n# Lớp InceptionBlock (điều chỉnh từ mã gốc)\nclass InceptionBlock(nn.Module):\n    def __init__(self, in_channels, out_channels=512):\n        super().__init__()\n        # Nhánh 1: 1x1\n        self.branch1x1 = nn.Sequential(\n            nn.Conv2d(in_channels, 128, kernel_size=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128)\n        )\n\n        # Nhánh 2: 1x1 -> 3x1 + 1x3\n        self.branch3x3 = nn.Sequential(\n            nn.Conv2d(in_channels, 96, kernel_size=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(96),\n            nn.Conv2d(96, 128, kernel_size=(3, 1), padding=(1, 0)),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, 128, kernel_size=(1, 3), padding=(0, 1)),\n            nn.ReLU(),\n            nn.BatchNorm2d(128)\n        )\n\n        # Nhánh 3: 1x1 -> 3x1 + 1x3 -> 3x1 + 1x3\n        self.branch5x5 = nn.Sequential(\n            nn.Conv2d(in_channels, 64, kernel_size=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 96, kernel_size=(3, 1), padding=(1, 0)),\n            nn.ReLU(),\n            nn.BatchNorm2d(96),\n            nn.Conv2d(96, 96, kernel_size=(1, 3), padding=(0, 1)),\n            nn.ReLU(),\n            nn.BatchNorm2d(96),\n            nn.Conv2d(96, 192, kernel_size=(3, 1), padding=(1, 0)),\n            nn.ReLU(),\n            nn.BatchNorm2d(192),\n            nn.Conv2d(192, 192, kernel_size=(1, 3), padding=(0, 1)),\n            nn.ReLU(),\n            nn.BatchNorm2d(192)\n        )\n\n        # Nhánh 4: MaxPool -> 1x1\n        self.branch_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(in_channels, 64, kernel_size=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64)\n        )\n\n        # Tầng cuối để điều chỉnh số kênh đầu ra\n        self.adjust_channels = nn.Conv2d(128 + 128 + 192 + 64, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        b1 = self.branch1x1(x)\n        b2 = self.branch3x3(x)\n        b3 = self.branch5x5(x)\n        b4 = self.branch_pool(x)\n        out = torch.cat([b1, b2, b3, b4], dim=1)\n        out = self.adjust_channels(out)\n        return out\n\n# Lớp PatchEmbedding (tối ưu với Conv2d)\nclass PatchEmbedding(nn.Module):\n    def __init__(self, in_channels, patch_size=7, emb_size=16):\n        super().__init__()\n        self.patch_size = patch_size\n        self.proj = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = self.proj(x)  # (B, emb_size, H/patch_size, W/patch_size)\n        B, C, H, W = x.shape\n        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)  # (B, num_patches, emb_size)\n        return x\n\n# Mô hình PlantXMamba với InceptionV3 pre-trained\nclass InceptionV3PlantXMamba(nn.Module):\n    def __init__(self, num_classes=38, patch_size=7, emb_size=16, d_state=16, d_conv=16, expand=4, n_layers=1, num_blocks=2, dropout=0.1):\n        super().__init__()\n\n        # InceptionV3 pre-trained\n        inception = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n        # Lấy các tầng đầu đến Mixed_6e (kích thước ~35x35, 768 kênh)\n        self.inception = nn.Sequential(*list(inception.children())[:12])  # Điều chỉnh số tầng nếu cần\n        # Điều chỉnh đầu ra thành (B, 512, 56, 56)\n        self.adjust_output = nn.Sequential(\n            nn.Conv2d(768, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            nn.Upsample(size=(56, 56), mode='bilinear', align_corners=False)\n        )\n\n        # Patch Embedding\n        self.patch_embed = PatchEmbedding(in_channels=512, patch_size=patch_size, emb_size=emb_size)\n\n        # Mamba blocks\n        mamba_args = type('Args', (), {\n            'd_model': emb_size,\n            'd_state': d_state,\n            'd_conv': d_conv,\n            'expand': expand,\n            'n_layers': n_layers,\n            'dropout': dropout\n        })()\n        self.mamba = nn.Sequential(*[MambaModule(mamba_args) for _ in range(num_blocks)])\n\n        # Classification head\n        self.norm = nn.LayerNorm(emb_size)\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.classifier = nn.Linear(emb_size, num_classes)\n\n    def forward(self, x):\n        x = self.inception(x)  # (B, 768, ~35, ~35)\n        x = self.adjust_output(x)  # (B, 512, 56, 56)\n        x = self.patch_embed(x)  # (B, 49, 16)\n        x = self.mamba(x)  # (B, 49, 16)\n        x = self.norm(x)  # (B, 49, 16)\n        x = x.permute(0, 2, 1)  # (B, 16, 49)\n        x = self.global_pool(x).squeeze(-1)  # (B, 16)\n        return self.classifier(x)  # (B, num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T07:25:10.802546Z","iopub.execute_input":"2025-06-29T07:25:10.802924Z","iopub.status.idle":"2025-06-29T07:25:10.818998Z","shell.execute_reply.started":"2025-06-29T07:25:10.802904Z","shell.execute_reply":"2025-06-29T07:25:10.818279Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"%cd PlantXViT\nroot_dir=\"./data/raw/plantvillage\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T07:25:15.132171Z","iopub.execute_input":"2025-06-29T07:25:15.132458Z","iopub.status.idle":"2025-06-29T07:25:15.137652Z","shell.execute_reply.started":"2025-06-29T07:25:15.132436Z","shell.execute_reply":"2025-06-29T07:25:15.137119Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/PlantXViT\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nimage_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(0.2, 0.2, 0.2, 0.0),\n    transforms.Resize((299, 299)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.ImageFolder(os.path.join(root_dir, \"train\"), transform=image_transforms)\nval_dataset = datasets.ImageFolder(os.path.join(root_dir, \"val\"), transform=image_transforms)\ntest_dataset = datasets.ImageFolder(os.path.join(root_dir, \"test\"), transform=image_transforms)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T07:25:16.398334Z","iopub.execute_input":"2025-06-29T07:25:16.399020Z","iopub.status.idle":"2025-06-29T07:25:16.528148Z","shell.execute_reply.started":"2025-06-29T07:25:16.398996Z","shell.execute_reply":"2025-06-29T07:25:16.527596Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom torchvision.models import inception_v3, Inception_V3_Weights\n\n# Transform (giảm phức tạp)\nimage_transforms = transforms.Compose([\n    transforms.Resize((299, 299)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# DataLoader\nroot_dir = \"./data/raw/plantvillage\"\ntrain_dataset = datasets.ImageFolder(os.path.join(root_dir, \"train\"), transform=image_transforms)\nval_dataset = datasets.ImageFolder(os.path.join(root_dir, \"val\"), transform=image_transforms)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=8, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=8, pin_memory=True)\n\n# Mô hình InceptionV3PlantXMamba\nclass InceptionV3PlantXMamba(nn.Module):\n    def __init__(self, num_classes=38, patch_size=14, emb_size=16, d_state=8, d_conv=8, expand=4, n_layers=1, num_blocks=2, dropout=0.1):\n        super().__init__()\n        inception = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n        self.inception = nn.Sequential(*list(inception.children())[:10])  # Đến Mixed_5d\n        self.adjust_output = nn.Sequential(\n            nn.Conv2d(288, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            nn.Upsample(size=(56, 56), mode='bilinear', align_corners=False)\n        )\n        self.patch_embed = PatchEmbedding(in_channels=512, patch_size=patch_size, emb_size=emb_size)\n        mamba_args = type('Args', (), {\n            'd_model': emb_size,\n            'd_state': d_state,\n            'd_conv': d_conv,\n            'expand': expand,\n            'n_layers': n_layers,\n            'dropout': dropout\n        })()\n        self.mamba = nn.Sequential(*[MambaModule(mamba_args) for _ in range(num_blocks)])\n        self.norm = nn.LayerNorm(emb_size)\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.classifier = nn.Linear(emb_size, num_classes)\n\n    def forward(self, x):\n        x = self.inception(x)\n        x = self.adjust_output(x)\n        x = self.patch_embed(x)\n        x = self.mamba(x)\n        x = self.norm(x)\n        x = x.permute(0, 2, 1)\n        x = self.global_pool(x).squeeze(-1)\n        return self.classifier(x)\n\n# Khởi tạo và tối ưu\nDEVICE = torch.device('cuda')\nmodel = InceptionV3PlantXMamba(num_classes=38, patch_size=14, d_state=8, d_conv=8).to(DEVICE)\nfor param in model.inception.parameters():\n    param.requires_grad = False  # Đóng băng InceptionV3\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=1e-4)\n\n# Hàm huấn luyện với mixed precision\ndef train_one_epoch(model, loader, optimizer, criterion):\n    model.train()\n    running_loss, correct, total = 0, 0, 0\n    scaler = GradScaler()\n    for inputs, labels in tqdm(loader, desc=\"Training\"):\n        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        with autocast():\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        running_loss += loss.item() * inputs.size(0)\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    avg_loss = running_loss / total\n    acc = correct / total\n    return avg_loss, acc\n\n# Hàm đánh giá (từ mã gốc)\ndef evaluate(model, loader, criterion):\n    model.eval()\n    running_loss, correct, total = 0, 0, 0\n    with torch.no_grad():\n        for inputs, labels in tqdm(loader, desc=\"Evaluating\"):\n            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n            with autocast():  # Thêm autocast để nhất quán với mixed precision\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    avg_loss = running_loss / total\n    acc = correct / total\n    return avg_loss, acc\n\n# Vòng lặp huấn luyện\nEPOCHS = 50\nMODEL_PATH = \"./outputs/plantVillage/models/InceptionMamba_296_plantvillage.pth\"\nbest_val_acc = 0\npatience, wait = 5, 0\nfor epoch in range(EPOCHS):\n    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n    val_loss, val_acc = evaluate(model, val_loader, criterion)\n    print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n    print(f\"Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), MODEL_PATH)\n        print(f\"✅ Saved best model to {MODEL_PATH}\")\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T07:25:21.284752Z","iopub.execute_input":"2025-06-29T07:25:21.285070Z","iopub.status.idle":"2025-06-29T08:06:41.607889Z","shell.execute_reply.started":"2025-06-29T07:25:21.285046Z","shell.execute_reply":"2025-06-29T08:06:41.607013Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\nDownloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n100%|██████████| 104M/104M [00:00<00:00, 224MB/s] \n/tmp/ipykernel_35/4157713021.py:72: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/2375 [00:00<?, ?it/s]/tmp/ipykernel_35/4157713021.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nTraining: 100%|██████████| 2375/2375 [01:59<00:00, 19.82it/s]\nEvaluating:   0%|          | 0/510 [00:00<?, ?it/s]/tmp/ipykernel_35/4157713021.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():  # Thêm autocast để nhất quán với mixed precision\nEvaluating: 100%|██████████| 510/510 [00:19<00:00, 26.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9526 | Acc: 0.6915\nVal   Loss: 1.0667 | Acc: 0.8500\n✅ Saved best model to ./outputs/plantVillage/models/InceptionMamba_296_plantvillage.pth\n\nEpoch 2/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:57<00:00, 20.15it/s]\nEvaluating: 100%|██████████| 510/510 [00:19<00:00, 26.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8398 | Acc: 0.8836\nVal   Loss: 0.4511 | Acc: 0.9247\n✅ Saved best model to ./outputs/plantVillage/models/InceptionMamba_296_plantvillage.pth\n\nEpoch 3/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.11it/s]\nEvaluating: 100%|██████████| 510/510 [00:19<00:00, 26.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3820 | Acc: 0.9453\nVal   Loss: 0.2384 | Acc: 0.9527\n✅ Saved best model to ./outputs/plantVillage/models/InceptionMamba_296_plantvillage.pth\n\nEpoch 4/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.03it/s]\nEvaluating: 100%|██████████| 510/510 [00:19<00:00, 26.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1908 | Acc: 0.9712\nVal   Loss: 0.1464 | Acc: 0.9632\n✅ Saved best model to ./outputs/plantVillage/models/InceptionMamba_296_plantvillage.pth\n\nEpoch 5/50\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 510/510 [00:19<00:00, 26.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1067 | Acc: 0.9837\nVal   Loss: 0.1231 | Acc: 0.9682\n✅ Saved best model to ./outputs/plantVillage/models/InceptionMamba_296_plantvillage.pth\n\nEpoch 6/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.02it/s]\nEvaluating: 100%|██████████| 510/510 [00:19<00:00, 26.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0676 | Acc: 0.9891\nVal   Loss: 0.1219 | Acc: 0.9637\n\nEpoch 7/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.01it/s]\nEvaluating: 100%|██████████| 510/510 [00:18<00:00, 26.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0458 | Acc: 0.9924\nVal   Loss: 0.0856 | Acc: 0.9742\n✅ Saved best model to ./outputs/plantVillage/models/InceptionMamba_296_plantvillage.pth\n\nEpoch 8/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.02it/s]\nEvaluating: 100%|██████████| 510/510 [00:18<00:00, 27.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0388 | Acc: 0.9928\nVal   Loss: 0.0909 | Acc: 0.9740\n\nEpoch 9/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:57<00:00, 20.14it/s]\nEvaluating: 100%|██████████| 510/510 [00:19<00:00, 26.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0324 | Acc: 0.9937\nVal   Loss: 0.0791 | Acc: 0.9780\n✅ Saved best model to ./outputs/plantVillage/models/InceptionMamba_296_plantvillage.pth\n\nEpoch 10/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:59<00:00, 19.85it/s]\nEvaluating: 100%|██████████| 510/510 [00:19<00:00, 26.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0303 | Acc: 0.9926\nVal   Loss: 0.0980 | Acc: 0.9714\n\nEpoch 11/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.10it/s]\nEvaluating: 100%|██████████| 510/510 [00:18<00:00, 27.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0262 | Acc: 0.9941\nVal   Loss: 0.0944 | Acc: 0.9732\n\nEpoch 12/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.09it/s]\nEvaluating: 100%|██████████| 510/510 [00:18<00:00, 26.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0210 | Acc: 0.9954\nVal   Loss: 0.0723 | Acc: 0.9783\n✅ Saved best model to ./outputs/plantVillage/models/InceptionMamba_296_plantvillage.pth\n\nEpoch 13/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.01it/s]\nEvaluating: 100%|██████████| 510/510 [00:19<00:00, 26.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0213 | Acc: 0.9953\nVal   Loss: 0.0657 | Acc: 0.9813\n✅ Saved best model to ./outputs/plantVillage/models/InceptionMamba_296_plantvillage.pth\n\nEpoch 14/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.04it/s]\nEvaluating: 100%|██████████| 510/510 [00:19<00:00, 26.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0202 | Acc: 0.9948\nVal   Loss: 0.0733 | Acc: 0.9799\n\nEpoch 15/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.05it/s]\nEvaluating: 100%|██████████| 510/510 [00:18<00:00, 26.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0164 | Acc: 0.9959\nVal   Loss: 0.0774 | Acc: 0.9802\n\nEpoch 16/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.03it/s]\nEvaluating: 100%|██████████| 510/510 [00:18<00:00, 26.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0169 | Acc: 0.9960\nVal   Loss: 0.0942 | Acc: 0.9745\n\nEpoch 17/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.04it/s]\nEvaluating: 100%|██████████| 510/510 [00:19<00:00, 26.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0169 | Acc: 0.9960\nVal   Loss: 0.1181 | Acc: 0.9693\n\nEpoch 18/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2375/2375 [01:58<00:00, 20.08it/s]\nEvaluating: 100%|██████████| 510/510 [00:19<00:00, 26.47it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0158 | Acc: 0.9962\nVal   Loss: 0.1020 | Acc: 0.9721\nEarly stopping at epoch 18\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-29T06:59:09.731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, accuracy_score\nfrom torch.nn.functional import softmax\nfrom torchvision import datasets, transforms\nimport os\n\n# # Khởi tạo mô hình\n# model = InceptionPlantXMamba(num_classes=38) \n# MODEL_PATH = \"./PlantXMamba/outputs/embrapa/models/VGGPlantXMamba_embrapa_27_6.pth\"\n# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# # Tải trọng số mô hình\n# model.load_state_dict(torch.load(MODEL_PATH))\n# model.to(DEVICE)\n# model.eval()\n\n# # Chuẩn bị dữ liệu kiểm tra\n# image_transforms = transforms.Compose([\n#     transforms.RandomHorizontalFlip(),\n#     transforms.ColorJitter(0.2, 0.2, 0.2, 0.0),\n#     transforms.Resize((224, 224)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n# ])\n\n# root_dir = './PlantXViT/data/raw/embrapa'\n# test_dataset = datasets.ImageFolder(os.path.join(root_dir, \"test\"), transform=image_transforms)\n# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n\n# Hàm đánh giá\ncriterion = nn.CrossEntropyLoss()\nall_preds = []\nall_labels = []\nall_probs = []\ntotal_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        total_loss += loss.item() * inputs.size(0)\n\n        # Lấy xác suất và nhãn dự đoán\n        probs = softmax(outputs, dim=1).cpu().numpy()  # Xác suất cho tất cả lớp\n        preds = torch.argmax(outputs, dim=1).cpu().numpy()  # Nhãn dự đoán\n        labels = labels.cpu().numpy()\n\n        all_preds.extend(preds)\n        all_labels.extend(labels)\n        all_probs.extend(probs)\n\n# Chuyển sang numpy array\nall_preds = np.array(all_preds)\nall_labels = np.array(all_labels)\nall_probs = np.array(all_probs)\n\n# Tính các chỉ số\nloss = total_loss / len(test_dataset)  # Mất mát trung bình\naccuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\nkappa = cohen_kappa_score(all_labels, all_preds)\n\n# AUC cho bài toán đa lớp (one-vs-rest)\nauc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')\n\n# In kết quả\nprint(f\"Loss (Mất mát): {loss:.4f}\")\nprint(f\"Accuracy (Độ chính xác): {accuracy:.4f}\")\nprint(f\"Precision (Độ chính xác dự đoán dương): {precision:.4f}\")\nprint(f\"Recall (Tỷ lệ phát hiện dương): {recall:.4f}\")\nprint(f\"F1 Score (Trung bình điều hòa): {f1:.4f}\")\nprint(f\"AUC (Diện tích dưới đường cong ROC): {auc:.4f}\")\nprint(f\"Kappa Score (Độ đo Cohen’s Kappa): {kappa:.4f}\")\n\n# # Lưu kết quả vào file\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:08:40.762799Z","iopub.execute_input":"2025-06-29T08:08:40.763543Z","iopub.status.idle":"2025-06-29T08:09:07.447855Z","shell.execute_reply.started":"2025-06-29T08:08:40.763511Z","shell.execute_reply":"2025-06-29T08:09:07.446928Z"}},"outputs":[{"name":"stdout","text":"Loss (Mất mát): 0.1445\nAccuracy (Độ chính xác): 0.9626\nPrecision (Độ chính xác dự đoán dương): 0.9649\nRecall (Tỷ lệ phát hiện dương): 0.9626\nF1 Score (Trung bình điều hòa): 0.9624\nAUC (Diện tích dưới đường cong ROC): 0.9994\nKappa Score (Độ đo Cohen’s Kappa): 0.9608\n","output_type":"stream"}],"execution_count":12}]}