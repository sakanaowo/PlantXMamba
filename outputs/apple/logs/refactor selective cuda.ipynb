{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM7vrjrwfABA",
        "outputId": "18529f88-b0d3-45fa-893c-7253d5f87e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-gjr_di57\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-gjr_di57\n",
            "  Resolved https://github.com/huggingface/transformers to commit 21cb353b7b4f77c6f5f5c3341d660f86ff416d04\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.0.dev0) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.0.dev0) (2025.6.15)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.53.0.dev0-py3-none-any.whl size=11446517 sha256=11b08c3bceb7b3f1a2350649dd9a1762d7bdd8f9e074917a0b251fe6b14783ae\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mw90oar5/wheels/04/a3/f1/b88775f8e1665827525b19ac7590250f1038d947067beba9fb\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "Successfully installed transformers-4.53.0.dev0\n",
            "Collecting git+https://github.com/huggingface/accelerate\n",
            "  Cloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-elih_gsj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-elih_gsj\n",
            "  Resolved https://github.com/huggingface/accelerate to commit 5987d79a538d2270deea1778e5625e869c4936b8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0.dev0) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0.dev0) (0.33.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate==1.9.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.9.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.9.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.9.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.9.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.9.0.dev0) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.9.0.dev0) (1.1.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate==1.9.0.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate==1.9.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.9.0.dev0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.9.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.9.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.9.0.dev0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.9.0.dev0) (2025.6.15)\n",
            "Building wheels for collected packages: accelerate\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for accelerate: filename=accelerate-1.9.0.dev0-py3-none-any.whl size=365407 sha256=d973fd1a55838ffabe7a70997673813215f855d4c1f51ccd7b63ec3265570270\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vd2z6f8z/wheels/18/af/f7/facfc4ea8d2484e23fc8489825221fe5826625fad79301dd99\n",
            "Successfully built accelerate\n",
            "Installing collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.7.0\n",
            "    Uninstalling accelerate-1.7.0:\n",
            "      Successfully uninstalled accelerate-1.7.0\n",
            "Successfully installed accelerate-1.9.0.dev0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install git+https://github.com/huggingface/accelerate\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/Dao-AILab/causal-conv1d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNibhNHSfiWU",
        "outputId": "e5fc80fc-406b-45e5-a595-a07d9c02104b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/Dao-AILab/causal-conv1d\n",
            "  Cloning https://github.com/Dao-AILab/causal-conv1d to /tmp/pip-req-build-ehe65pp4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Dao-AILab/causal-conv1d /tmp/pip-req-build-ehe65pp4\n",
            "  Resolved https://github.com/Dao-AILab/causal-conv1d to commit e940ead2fd962c56854455017541384909ca669f\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from causal_conv1d==1.5.0.post8) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from causal_conv1d==1.5.0.post8) (24.2)\n",
            "Collecting ninja (from causal_conv1d==1.5.0.post8)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->causal_conv1d==1.5.0.post8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->causal_conv1d==1.5.0.post8) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->causal_conv1d==1.5.0.post8) (3.0.2)\n",
            "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: causal_conv1d\n",
            "  Building wheel for causal_conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for causal_conv1d: filename=causal_conv1d-1.5.0.post8-cp311-cp311-linux_x86_64.whl size=103984203 sha256=f1bff4a48017e4a1b2f18a84b3d881df7de2cff8cc9dbbc0c53c9d5df233359b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t7b6gp4j/wheels/78/92/4d/c421024d7eef5e9a7a05dc488f090c3bddb325455544821101\n",
            "Successfully built causal_conv1d\n",
            "Installing collected packages: ninja, causal_conv1d\n",
            "Successfully installed causal_conv1d-1.5.0.post8 ninja-1.11.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/state-spaces/mamba.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGY18L8NgEIc",
        "outputId": "5ca28a2d-cb1f-4ead-8cff-dd994f043f19"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mamba'...\n",
            "remote: Enumerating objects: 739, done.\u001b[K\n",
            "remote: Total 739 (delta 0), reused 0 (delta 0), pack-reused 739 (from 1)\u001b[K\n",
            "Receiving objects: 100% (739/739), 1.55 MiB | 4.17 MiB/s, done.\n",
            "Resolving deltas: 100% (405/405), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mamba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBX4sxXfgw1H",
        "outputId": "f594c009-3b26-4558-89e3-e26cc7b9bdff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mamba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export CAUSAL_CONV1D_FORCE_BUILD=TRUE\n",
        "!export CAUSAL_CONV1D_SKIP_CUDA_BUILD=TRUE\n",
        "!export CAUSAL_CONV1D_FORCE_CXX11_ABI=TRUE"
      ],
      "metadata": {
        "id": "KcRvtFmshXtc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show causal-conv1d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQDoIuRAsq8l",
        "outputId": "6656a932-0e9d-44f6-97ab-7ab82851a04e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: causal-conv1d\n",
            "Version: 1.5.0.post8\n",
            "Summary: Causal depthwise conv1d in CUDA, with a PyTorch interface\n",
            "Home-page: https://github.com/Dao-AILab/causal-conv1d\n",
            "Author: Tri Dao\n",
            "Author-email: tri@tridao.me\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: ninja, packaging, torch\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install . --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0Jo6QEwg27x",
        "outputId": "9f1a4356-e717-445b-e0ac-01688b5aba1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/mamba\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mamba_ssm==2.2.4) (2.6.0+cu124)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from mamba_ssm==2.2.4) (3.2.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from mamba_ssm==2.2.4) (1.11.1.4)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from mamba_ssm==2.2.4) (0.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from mamba_ssm==2.2.4) (4.53.0.dev0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mamba_ssm==2.2.4) (24.2)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.11/dist-packages (from mamba_ssm==2.2.4) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm==2.2.4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->mamba_ssm==2.2.4) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm==2.2.4) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm==2.2.4) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm==2.2.4) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm==2.2.4) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm==2.2.4) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm==2.2.4) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm==2.2.4) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm==2.2.4) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->mamba_ssm==2.2.4) (1.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->mamba_ssm==2.2.4) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba_ssm==2.2.4) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba_ssm==2.2.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba_ssm==2.2.4) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba_ssm==2.2.4) (2025.6.15)\n",
            "Building wheels for collected packages: mamba_ssm\n",
            "  Building wheel for mamba_ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba_ssm: filename=mamba_ssm-2.2.4-cp311-cp311-linux_x86_64.whl size=323672993 sha256=8a0be01153fa30727a9e69024fbe061eb92c7ba4416d2049c5fc3107ed91d852\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-49t1ohmp/wheels/d1/52/ed/0424436ff562924889b52e36e9848d82b09b3dfd1c3c675f47\n",
            "Successfully built mamba_ssm\n",
            "Installing collected packages: mamba_ssm\n",
            "Successfully installed mamba_ssm-2.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.models import VGG16_Weights\n",
        "from mamba_ssm import Mamba\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels=128):\n",
        "        super(InceptionBlock, self).__init__()\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 128, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128)\n",
        "        )\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 96, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.Conv2d(96, 128, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128)\n",
        "        )\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 96, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.Conv2d(96, 96, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.Conv2d(96, 192, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.Conv2d(192, 192, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(192)\n",
        "        )\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.branch1x1(x)\n",
        "        b2 = self.branch3x3(x)\n",
        "        b3 = self.branch5x5(x)\n",
        "        b4 = self.branch_pool(x)\n",
        "        return torch.cat([b1, b2, b3, b4], dim=1)\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size=5, emb_size=16):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.emb_size = emb_size\n",
        "        self.proj = nn.Linear(in_channels * patch_size * patch_size, emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
        "        x = x.view(B, -1, C * self.patch_size * self.patch_size)\n",
        "        return self.proj(x)\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, emb_size=16, d_state=64, d_conv=4, expand=4):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "        self.mamba = Mamba(\n",
        "            d_model=emb_size,\n",
        "            d_state=d_state,\n",
        "            d_conv=d_conv,\n",
        "            expand=expand\n",
        "        )\n",
        "        print(\"Mamba instance:\", self.mamba)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_size, emb_size * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(emb_size * 2, emb_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.norm(x)\n",
        "        print(\"Input to Mamba:\", x.shape)\n",
        "        print(\"Mamba callable:\", callable(self.mamba))\n",
        "        print(\"Type of conv1d inside Mamba:\", type(self.mamba.conv1d))\n",
        "        x = self.mamba(x)\n",
        "        x = residual + x\n",
        "        x = x + self.mlp(self.norm(x))\n",
        "        return self.dropout(x)\n",
        "\n",
        "class PlantXMamba(nn.Module):\n",
        "    def __init__(self, num_classes=4, patch_size=5, emb_size=16, num_blocks=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
        "        self.vgg_block = nn.Sequential(*list(vgg.features[:10]))\n",
        "        self.inception = InceptionBlock(in_channels=128)\n",
        "        self.patch_embed = PatchEmbedding(in_channels=512, patch_size=patch_size, emb_size=emb_size)\n",
        "        self.transformer = nn.Sequential(*[MambaBlock(emb_size=emb_size, d_state=64, d_conv=4, expand=4) for _ in range(num_blocks)])\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vgg_block(x)\n",
        "        x = self.inception(x)\n",
        "        x = self.patch_embed(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.norm(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.global_pool(x).squeeze(-1)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "kvnCESm3tAuL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_plantxmamba(batch_size, device, input_size=(3, 224, 224), num_classes=4, patch_size=5, emb_size=16):\n",
        "    print(f\"Testing PlantXMamba with batch_size={batch_size} on device={device}\")\n",
        "\n",
        "    model = PlantXMamba(\n",
        "        num_classes=num_classes,\n",
        "        patch_size=patch_size,\n",
        "        emb_size=emb_size,\n",
        "        num_blocks=4,\n",
        "        dropout=0.1\n",
        "    ).to(device)\n",
        "\n",
        "    model_device = next(model.parameters()).device\n",
        "    print(f\"Model is on device: {model_device}\")\n",
        "\n",
        "    x = torch.randn(batch_size, *input_size).to(device)\n",
        "    print(f\"Input shape: {x.shape}, Input device: {x.device}\")\n",
        "\n",
        "    output = None\n",
        "    try:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(x)\n",
        "        print(f\"Output shape: {output.shape}\")\n",
        "        print(f\"Output device: {output.device}\")\n",
        "\n",
        "        expected_output_shape = (batch_size, num_classes)\n",
        "        assert output.shape == expected_output_shape, \\\n",
        "            f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n",
        "        print(\"Test passed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during testing: {str(e)}\")\n",
        "\n",
        "    return output\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    print(\"\\n--- Test Case 1: Batch size = 1 ---\")\n",
        "    output1 = test_plantxmamba(batch_size=1, device=device, patch_size=5, emb_size=16)\n",
        "\n",
        "    print(\"\\n--- Test Case 2: Batch size = 4 ---\")\n",
        "    output2 = test_plantxmamba(batch_size=4, device=device, patch_size=5, emb_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RO-FN3h5tGuU",
        "outputId": "013e3190-d7db-42c4-8f96-2e7b92ed1c61"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "--- Test Case 1: Batch size = 1 ---\n",
            "Testing PlantXMamba with batch_size=1 on device=cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 75.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mamba instance: Mamba(\n",
            "  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n",
            "  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
            "  (act): SiLU()\n",
            "  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n",
            "  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n",
            "  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n",
            ")\n",
            "Mamba instance: Mamba(\n",
            "  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n",
            "  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
            "  (act): SiLU()\n",
            "  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n",
            "  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n",
            "  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n",
            ")\n",
            "Mamba instance: Mamba(\n",
            "  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n",
            "  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
            "  (act): SiLU()\n",
            "  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n",
            "  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n",
            "  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n",
            ")\n",
            "Mamba instance: Mamba(\n",
            "  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n",
            "  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
            "  (act): SiLU()\n",
            "  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n",
            "  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n",
            "  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n",
            ")\n",
            "Model is on device: cuda:0\n",
            "Input shape: torch.Size([1, 3, 224, 224]), Input device: cuda:0\n",
            "Input to Mamba: torch.Size([1, 121, 16])\n",
            "Mamba callable: True\n",
            "Type of conv1d inside Mamba: <class 'torch.nn.modules.conv.Conv1d'>\n",
            "Input to Mamba: torch.Size([1, 121, 16])\n",
            "Mamba callable: True\n",
            "Type of conv1d inside Mamba: <class 'torch.nn.modules.conv.Conv1d'>\n",
            "Input to Mamba: torch.Size([1, 121, 16])\n",
            "Mamba callable: True\n",
            "Type of conv1d inside Mamba: <class 'torch.nn.modules.conv.Conv1d'>\n",
            "Input to Mamba: torch.Size([1, 121, 16])\n",
            "Mamba callable: True\n",
            "Type of conv1d inside Mamba: <class 'torch.nn.modules.conv.Conv1d'>\n",
            "Output shape: torch.Size([1, 4])\n",
            "Output device: cuda:0\n",
            "Test passed successfully!\n",
            "\n",
            "--- Test Case 2: Batch size = 4 ---\n",
            "Testing PlantXMamba with batch_size=4 on device=cuda\n",
            "Mamba instance: Mamba(\n",
            "  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n",
            "  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
            "  (act): SiLU()\n",
            "  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n",
            "  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n",
            "  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n",
            ")\n",
            "Mamba instance: Mamba(\n",
            "  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n",
            "  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
            "  (act): SiLU()\n",
            "  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n",
            "  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n",
            "  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n",
            ")\n",
            "Mamba instance: Mamba(\n",
            "  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n",
            "  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
            "  (act): SiLU()\n",
            "  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n",
            "  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n",
            "  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n",
            ")\n",
            "Mamba instance: Mamba(\n",
            "  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n",
            "  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
            "  (act): SiLU()\n",
            "  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n",
            "  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n",
            "  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n",
            ")\n",
            "Model is on device: cuda:0\n",
            "Input shape: torch.Size([4, 3, 224, 224]), Input device: cuda:0\n",
            "Input to Mamba: torch.Size([4, 121, 16])\n",
            "Mamba callable: True\n",
            "Type of conv1d inside Mamba: <class 'torch.nn.modules.conv.Conv1d'>\n",
            "Input to Mamba: torch.Size([4, 121, 16])\n",
            "Mamba callable: True\n",
            "Type of conv1d inside Mamba: <class 'torch.nn.modules.conv.Conv1d'>\n",
            "Input to Mamba: torch.Size([4, 121, 16])\n",
            "Mamba callable: True\n",
            "Type of conv1d inside Mamba: <class 'torch.nn.modules.conv.Conv1d'>\n",
            "Input to Mamba: torch.Size([4, 121, 16])\n",
            "Mamba callable: True\n",
            "Type of conv1d inside Mamba: <class 'torch.nn.modules.conv.Conv1d'>\n",
            "Output shape: torch.Size([4, 4])\n",
            "Output device: cuda:0\n",
            "Test passed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELHnD-NmvyOt",
        "outputId": "79be3b73-3658-47a1-8e40-be09921c0ef7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcwcSI4F1rKx",
        "outputId": "7891949c-35b5-4c7c-9a75-78949e078142"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mamba  sample_data  selective_scan_interface.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/selective_scan_interface.py /content/mamba/mamba_ssm/ops/selective_scan_interface.py"
      ],
      "metadata": {
        "id": "uiMz9qq3wSXY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ./mamba/mamba_ssm/ops/selective_scan_interface.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K5ZbApfP10RG",
        "outputId": "babc4187-0d36-4398-cb77-808deb55e06f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Copyright (c) 2023, Tri Dao, Albert Gu.\n",
            "\n",
            "import torch\n",
            "import torch.nn.functional as F\n",
            "from mamba_ssm.utils.torch import custom_bwd, custom_fwd\n",
            "\n",
            "from einops import rearrange, repeat\n",
            "\n",
            "try:\n",
            "    from causal_conv1d import causal_conv1d_fn\n",
            "    from causal_conv1d.cpp_functions import causal_conv1d_fwd_function, causal_conv1d_bwd_function, \\\n",
            "        causal_conv1d_update_function\n",
            "except ImportError:\n",
            "    causal_conv1d_fn = None\n",
            "    causal_conv1d_fwd_function = None\n",
            "    causal_conv1d_bwd_function = None\n",
            "    causal_conv1d_update_function = None\n",
            "\n",
            "from mamba_ssm.ops.triton.layer_norm import _layer_norm_fwd\n",
            "\n",
            "import selective_scan_cuda\n",
            "\n",
            "\n",
            "class SelectiveScanFn(torch.autograd.Function):\n",
            "\n",
            "    @staticmethod\n",
            "    def forward(ctx, u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False,\n",
            "                return_last_state=False):\n",
            "        if u.stride(-1) != 1:\n",
            "            u = u.contiguous()\n",
            "        if delta.stride(-1) != 1:\n",
            "            delta = delta.contiguous()\n",
            "        if D is not None:\n",
            "            D = D.contiguous()\n",
            "        if B.stride(-1) != 1:\n",
            "            B = B.contiguous()\n",
            "        if C.stride(-1) != 1:\n",
            "            C = C.contiguous()\n",
            "        if z is not None and z.stride(-1) != 1:\n",
            "            z = z.contiguous()\n",
            "        if B.dim() == 3:\n",
            "            B = rearrange(B, \"b dstate l -> b 1 dstate l\")\n",
            "            ctx.squeeze_B = True\n",
            "        if C.dim() == 3:\n",
            "            C = rearrange(C, \"b dstate l -> b 1 dstate l\")\n",
            "            ctx.squeeze_C = True\n",
            "        out, x, *rest = selective_scan_cuda.fwd(u, delta, A, B, C, D, z, delta_bias, delta_softplus)\n",
            "        ctx.delta_softplus = delta_softplus\n",
            "        ctx.has_z = z is not None\n",
            "        last_state = x[:, :, -1, 1::2]  # (batch, dim, dstate)\n",
            "        if not ctx.has_z:\n",
            "            ctx.save_for_backward(u, delta, A, B, C, D, delta_bias, x)\n",
            "            return out if not return_last_state else (out, last_state)\n",
            "        else:\n",
            "            ctx.save_for_backward(u, delta, A, B, C, D, z, delta_bias, x, out)\n",
            "            out_z = rest[0]\n",
            "            return out_z if not return_last_state else (out_z, last_state)\n",
            "\n",
            "    @staticmethod\n",
            "    def backward(ctx, dout, *args):\n",
            "        if not ctx.has_z:\n",
            "            u, delta, A, B, C, D, delta_bias, x = ctx.saved_tensors\n",
            "            z = None\n",
            "            out = None\n",
            "        else:\n",
            "            u, delta, A, B, C, D, z, delta_bias, x, out = ctx.saved_tensors\n",
            "        if dout.stride(-1) != 1:\n",
            "            dout = dout.contiguous()\n",
            "        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the\n",
            "        # backward of selective_scan_cuda with the backward of chunk).\n",
            "        # Here we just pass in None and dz will be allocated in the C++ code.\n",
            "        du, ddelta, dA, dB, dC, dD, ddelta_bias, *rest = selective_scan_cuda.bwd(\n",
            "            u, delta, A, B, C, D, z, delta_bias, dout, x, out, None, ctx.delta_softplus,\n",
            "            False  # option to recompute out_z, not used here\n",
            "        )\n",
            "        dz = rest[0] if ctx.has_z else None\n",
            "        dB = dB.squeeze(1) if getattr(ctx, \"squeeze_B\", False) else dB\n",
            "        dC = dC.squeeze(1) if getattr(ctx, \"squeeze_C\", False) else dC\n",
            "        return (du, ddelta, dA, dB, dC,\n",
            "                dD if D is not None else None,\n",
            "                dz,\n",
            "                ddelta_bias if delta_bias is not None else None,\n",
            "                None,\n",
            "                None)\n",
            "\n",
            "\n",
            "def rms_norm_forward(\n",
            "        x,\n",
            "        weight,\n",
            "        bias,\n",
            "        eps=1e-6,\n",
            "        is_rms_norm=True,\n",
            "):\n",
            "    # x (b l) d\n",
            "    if x.stride(-1) != 1:\n",
            "        x = x.contiguous()\n",
            "    weight = weight.contiguous()\n",
            "    if bias is not None:\n",
            "        bias = bias.contiguous()\n",
            "    y = _layer_norm_fwd(\n",
            "        x, weight, bias, eps, None, residual_dtype=None, is_rms_norm=is_rms_norm\n",
            "    )[0]\n",
            "    # y (b l) d\n",
            "    return y\n",
            "\n",
            "\n",
            "def selective_scan_fn(u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False,\n",
            "                      return_last_state=False):\n",
            "    \"\"\"if return_last_state is True, returns (out, last_state)\n",
            "    last_state has shape (batch, dim, dstate). Note that the gradient of the last state is\n",
            "    not considered in the backward pass.\n",
            "    \"\"\"\n",
            "    return SelectiveScanFn.apply(u, delta, A, B, C, D, z, delta_bias, delta_softplus, return_last_state)\n",
            "\n",
            "\n",
            "def selective_scan_ref(u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False,\n",
            "                       return_last_state=False):\n",
            "    \"\"\"\n",
            "    u: r(B D L)\n",
            "    delta: r(B D L)\n",
            "    A: c(D N) or r(D N)\n",
            "    B: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)\n",
            "    C: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)\n",
            "    D: r(D)\n",
            "    z: r(B D L)\n",
            "    delta_bias: r(D), fp32\n",
            "\n",
            "    out: r(B D L)\n",
            "    last_state (optional): r(B D dstate) or c(B D dstate)\n",
            "    \"\"\"\n",
            "    dtype_in = u.dtype\n",
            "    u = u.float()\n",
            "    delta = delta.float()\n",
            "    if delta_bias is not None:\n",
            "        delta = delta + delta_bias[..., None].float()\n",
            "    if delta_softplus:\n",
            "        delta = F.softplus(delta)\n",
            "    batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]\n",
            "    is_variable_B = B.dim() >= 3\n",
            "    is_variable_C = C.dim() >= 3\n",
            "    if A.is_complex():\n",
            "        if is_variable_B:\n",
            "            B = torch.view_as_complex(rearrange(B.float(), \"... (L two) -> ... L two\", two=2))\n",
            "        if is_variable_C:\n",
            "            C = torch.view_as_complex(rearrange(C.float(), \"... (L two) -> ... L two\", two=2))\n",
            "    else:\n",
            "        B = B.float()\n",
            "        C = C.float()\n",
            "    x = A.new_zeros((batch, dim, dstate))\n",
            "    ys = []\n",
            "    deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n",
            "    if not is_variable_B:\n",
            "        deltaB_u = torch.einsum('bdl,dn,bdl->bdln', delta, B, u)\n",
            "    else:\n",
            "        if B.dim() == 3:\n",
            "            deltaB_u = torch.einsum('bdl,bnl,bdl->bdln', delta, B, u)\n",
            "        else:\n",
            "            B = repeat(B, \"B G N L -> B (G H) N L\", H=dim // B.shape[1])\n",
            "            deltaB_u = torch.einsum('bdl,bdnl,bdl->bdln', delta, B, u)\n",
            "    if is_variable_C and C.dim() == 4:\n",
            "        C = repeat(C, \"B G N L -> B (G H) N L\", H=dim // C.shape[1])\n",
            "    last_state = None\n",
            "    for i in range(u.shape[2]):\n",
            "        x = deltaA[:, :, i] * x + deltaB_u[:, :, i]\n",
            "        if not is_variable_C:\n",
            "            y = torch.einsum('bdn,dn->bd', x, C)\n",
            "        else:\n",
            "            if C.dim() == 3:\n",
            "                y = torch.einsum('bdn,bn->bd', x, C[:, :, i])\n",
            "            else:\n",
            "                y = torch.einsum('bdn,bdn->bd', x, C[:, :, :, i])\n",
            "        if i == u.shape[2] - 1:\n",
            "            last_state = x\n",
            "        if y.is_complex():\n",
            "            y = y.real * 2\n",
            "        ys.append(y)\n",
            "    y = torch.stack(ys, dim=2)  # (batch dim L)\n",
            "    out = y if D is None else y + u * rearrange(D, \"d -> d 1\")\n",
            "    if z is not None:\n",
            "        out = out * F.silu(z)\n",
            "    out = out.to(dtype=dtype_in)\n",
            "    return out if not return_last_state else (out, last_state)\n",
            "\n",
            "\n",
            "class MambaInnerFn(torch.autograd.Function):\n",
            "\n",
            "    @staticmethod\n",
            "    @custom_fwd\n",
            "    def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
            "                out_proj_weight, out_proj_bias,\n",
            "                A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
            "                C_proj_bias=None, delta_softplus=True, checkpoint_lvl=1, b_rms_weight=None, c_rms_weight=None,\n",
            "                dt_rms_weight=None, b_c_dt_rms_eps=1e-6):\n",
            "        \"\"\"\n",
            "             xz: (batch, dim, seqlen)\n",
            "        \"\"\"\n",
            "        from causal_conv1d import causal_conv1d_fn\n",
            "        causal_conv1d_fwd_function = causal_conv1d_fn\n",
            "        assert causal_conv1d_fwd_function is not None, \"causal_conv1d_cuda is not available. Please install causal-conv1d.\"\n",
            "        assert checkpoint_lvl in [0, 1]\n",
            "        L = xz.shape[-1]\n",
            "        delta_rank = delta_proj_weight.shape[1]\n",
            "        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
            "        if torch.is_autocast_enabled():\n",
            "            x_proj_weight = x_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
            "            delta_proj_weight = delta_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
            "            out_proj_weight = out_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
            "            out_proj_bias = (out_proj_bias.to(dtype=torch.get_autocast_gpu_dtype())\n",
            "                             if out_proj_bias is not None else None)\n",
            "        if xz.stride(-1) != 1:\n",
            "            xz = xz.contiguous()\n",
            "        conv1d_weight = rearrange(conv1d_weight, \"d 1 w -> d w\")\n",
            "        x, z = xz.chunk(2, dim=1)\n",
            "        conv1d_bias = conv1d_bias.contiguous() if conv1d_bias is not None else None\n",
            "        conv1d_out = causal_conv1d_fwd_function(\n",
            "            x, conv1d_weight, conv1d_bias, None, None, None, True\n",
            "        )\n",
            "        # We're being very careful here about the layout, to avoid extra transposes.\n",
            "        # We want delta to have d as the slowest moving dimension\n",
            "        # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
            "        x_dbl = F.linear(rearrange(conv1d_out, 'b d l -> (b l) d'), x_proj_weight)  # (bl d)\n",
            "        delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(), \"d (b l) -> b d l\", l=L)\n",
            "        ctx.is_variable_B = B is None\n",
            "        ctx.is_variable_C = C is None\n",
            "        ctx.B_proj_bias_is_None = B_proj_bias is None\n",
            "        ctx.C_proj_bias_is_None = C_proj_bias is None\n",
            "        if B is None:  # variable B\n",
            "            B = x_dbl[:, delta_rank:delta_rank + d_state]  # (bl dstate)\n",
            "            if B_proj_bias is not None:\n",
            "                B = B + B_proj_bias.to(dtype=B.dtype)\n",
            "            if not A.is_complex():\n",
            "                # B = rearrange(B, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
            "                B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
            "            else:\n",
            "                B = rearrange(B, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n",
            "        else:\n",
            "            if B.stride(-1) != 1:\n",
            "                B = B.contiguous()\n",
            "        if C is None:  # variable C\n",
            "            C = x_dbl[:, -d_state:]  # (bl dstate)\n",
            "            if C_proj_bias is not None:\n",
            "                C = C + C_proj_bias.to(dtype=C.dtype)\n",
            "            if not A.is_complex():\n",
            "                # C = rearrange(C, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
            "                C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
            "            else:\n",
            "                C = rearrange(C, \"(b l) (dstate two) -> b 1 dstate (l two)\", l=L, two=2).contiguous()\n",
            "        else:\n",
            "            if C.stride(-1) != 1:\n",
            "                C = C.contiguous()\n",
            "        if D is not None:\n",
            "            D = D.contiguous()\n",
            "\n",
            "        if b_rms_weight is not None:\n",
            "            B = rearrange(B, \"b 1 dstate l -> (b l) dstate\", l=L).contiguous()\n",
            "            B = rms_norm_forward(B, b_rms_weight, bias=None, eps=b_c_dt_rms_eps)\n",
            "            B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
            "        if c_rms_weight is not None:\n",
            "            C = rearrange(C, \"b 1 dstate l -> (b l) dstate\", l=L).contiguous()\n",
            "            C = rms_norm_forward(C, c_rms_weight, bias=None, eps=b_c_dt_rms_eps)\n",
            "            C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
            "        if dt_rms_weight is not None:\n",
            "            delta = rearrange(delta, \"b d l -> (b l) d\", l=L).contiguous()\n",
            "            delta = rms_norm_forward(delta, dt_rms_weight, bias=None, eps=b_c_dt_rms_eps)\n",
            "            delta = rearrange(delta, \"(b l) d -> b d l\", l=L).contiguous()\n",
            "\n",
            "        out, scan_intermediates, out_z = selective_scan_cuda.fwd(\n",
            "            conv1d_out, delta, A, B, C, D, z, delta_bias, delta_softplus\n",
            "        )\n",
            "        ctx.delta_softplus = delta_softplus\n",
            "        ctx.out_proj_bias_is_None = out_proj_bias is None\n",
            "        ctx.checkpoint_lvl = checkpoint_lvl\n",
            "        ctx.b_rms_weight = b_rms_weight\n",
            "        ctx.c_rms_weight = c_rms_weight\n",
            "        ctx.dt_rms_weight = dt_rms_weight\n",
            "        ctx.b_c_dt_rms_eps = b_c_dt_rms_eps\n",
            "        if checkpoint_lvl >= 1:  # Will recompute conv1d_out and delta in the backward pass\n",
            "            conv1d_out, delta = None, None\n",
            "        ctx.save_for_backward(xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight,\n",
            "                              delta_proj_weight, out_proj_weight, conv1d_out, delta,\n",
            "                              A, B, C, D, delta_bias, scan_intermediates, b_rms_weight, c_rms_weight, dt_rms_weight,\n",
            "                              out)\n",
            "        return F.linear(rearrange(out_z, \"b d l -> b l d\"), out_proj_weight, out_proj_bias)\n",
            "\n",
            "    @staticmethod\n",
            "    @custom_bwd\n",
            "    def backward(ctx, dout):\n",
            "        # dout: (batch, seqlen, dim)\n",
            "        assert causal_conv1d_fwd_function is not None, \"causal_conv1d_cuda is not available. Please install causal-conv1d.\"\n",
            "        (xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight, delta_proj_weight, out_proj_weight,\n",
            "         conv1d_out, delta, A, B, C, D, delta_bias, scan_intermediates, b_rms_weight, c_rms_weight, dt_rms_weight,\n",
            "         out) = ctx.saved_tensors\n",
            "        L = xz.shape[-1]\n",
            "        delta_rank = delta_proj_weight.shape[1]\n",
            "        d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
            "        x, z = xz.chunk(2, dim=1)\n",
            "        if dout.stride(-1) != 1:\n",
            "            dout = dout.contiguous()\n",
            "        if ctx.checkpoint_lvl == 1:\n",
            "            conv1d_out = causal_conv1d_fwd_function(\n",
            "                x, conv1d_weight, conv1d_bias, None, None, None, True\n",
            "            )\n",
            "            delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(),\n",
            "                              \"d (b l) -> b d l\", l=L)\n",
            "            if dt_rms_weight is not None:\n",
            "                delta = rearrange(delta, \"b d l -> (b l) d\", l=L).contiguous()\n",
            "                delta = rms_norm_forward(delta, ctx.dt_rms_weight, None, ctx.b_c_dt_rms_eps)\n",
            "                delta = rearrange(delta, \"(b l) d -> b d l\", l=L).contiguous()\n",
            "            if b_rms_weight is not None:\n",
            "                # Recompute & RMSNorm B\n",
            "                B = rearrange(B, \"b 1 dstate l -> (b l) dstate\", l=L).contiguous()\n",
            "                B = rms_norm_forward(\n",
            "                    B, ctx.b_rms_weight, None, ctx.b_c_dt_rms_eps\n",
            "                )\n",
            "                B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
            "            if c_rms_weight is not None:\n",
            "                # Recompute & RMSNorm C\n",
            "                C = rearrange(C, \"b 1 dstate l -> (b l) dstate\", l=L).contiguous()\n",
            "                C = rms_norm_forward(\n",
            "                    C, ctx.c_rms_weight, None, ctx.b_c_dt_rms_eps\n",
            "                )\n",
            "                C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l=L).contiguous()\n",
            "\n",
            "        # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the\n",
            "        # backward of selective_scan_cuda with the backward of chunk).\n",
            "        dxz = torch.empty_like(xz)  # (batch, dim, seqlen)\n",
            "        dx, dz = dxz.chunk(2, dim=1)\n",
            "        dout = rearrange(dout, \"b l e -> e (b l)\")\n",
            "        dout_y = rearrange(out_proj_weight.t() @ dout, \"d (b l) -> b d l\", l=L)\n",
            "        dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z = selective_scan_cuda.bwd(\n",
            "            conv1d_out, delta, A, B, C, D, z, delta_bias, dout_y, scan_intermediates, out, dz,\n",
            "            ctx.delta_softplus,\n",
            "            True  # option to recompute out_z\n",
            "        )\n",
            "        dout_proj_weight = torch.einsum(\"eB,dB->ed\", dout, rearrange(out_z, \"b d l -> d (b l)\"))\n",
            "        dout_proj_bias = dout.sum(dim=(0, 1)) if not ctx.out_proj_bias_is_None else None\n",
            "        dD = dD if D is not None else None\n",
            "        dx_dbl = torch.empty_like(x_dbl)\n",
            "        dB_proj_bias = None\n",
            "        if ctx.is_variable_B:\n",
            "            if not A.is_complex():\n",
            "                dB = rearrange(dB, \"b 1 dstate l -> (b l) dstate\").contiguous()\n",
            "            else:\n",
            "                dB = rearrange(dB, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n",
            "            dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None\n",
            "            dx_dbl[:, delta_rank:delta_rank + d_state] = dB  # (bl d)\n",
            "            dB = None\n",
            "        dC_proj_bias = None\n",
            "        if ctx.is_variable_C:\n",
            "            if not A.is_complex():\n",
            "                dC = rearrange(dC, \"b 1 dstate l -> (b l) dstate\").contiguous()\n",
            "            else:\n",
            "                dC = rearrange(dC, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n",
            "            dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None\n",
            "            dx_dbl[:, -d_state:] = dC  # (bl d)\n",
            "            dC = None\n",
            "        ddelta = rearrange(ddelta, \"b d l -> d (b l)\")\n",
            "        ddelta_proj_weight = torch.einsum(\"dB,Br->dr\", ddelta, x_dbl[:, :delta_rank])\n",
            "        dx_dbl[:, :delta_rank] = torch.einsum(\"dB,dr->Br\", ddelta, delta_proj_weight)\n",
            "        dconv1d_out = rearrange(dconv1d_out, \"b d l -> d (b l)\")\n",
            "        dx_proj_weight = torch.einsum(\"Br,Bd->rd\", dx_dbl, rearrange(conv1d_out, \"b d l -> (b l) d\"))\n",
            "        dconv1d_out = torch.addmm(dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out)\n",
            "        dconv1d_out = rearrange(dconv1d_out, \"d (b l) -> b d l\", b=x.shape[0], l=x.shape[-1])\n",
            "        # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the\n",
            "        # backward of conv1d with the backward of chunk).\n",
            "        dx, dconv1d_weight, dconv1d_bias, *_ = causal_conv1d_bwd_function(\n",
            "            x, conv1d_weight, conv1d_bias, dconv1d_out, None, None, None, dx, False, True\n",
            "        )\n",
            "        dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None\n",
            "        dconv1d_weight = rearrange(dconv1d_weight, \"d w -> d 1 w\")\n",
            "        return (dxz, dconv1d_weight, dconv1d_bias, dx_proj_weight, ddelta_proj_weight,\n",
            "                dout_proj_weight, dout_proj_bias,\n",
            "                dA, dB, dC, dD,\n",
            "                ddelta_bias if delta_bias is not None else None,\n",
            "                # 6-None are delta_softplus, checkpoint_lvl, b_rms_weight, c_rms_weight, dt_rms_weight, b_c_dt_rms_eps\n",
            "                dB_proj_bias, dC_proj_bias, None, None, None, None, None, None)\n",
            "\n",
            "\n",
            "def mamba_inner_fn(\n",
            "        xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
            "        out_proj_weight, out_proj_bias,\n",
            "        A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
            "        C_proj_bias=None, delta_softplus=True, checkpoint_lvl=1, b_rms_weight=None, c_rms_weight=None,\n",
            "        dt_rms_weight=None, b_c_dt_rms_eps=1e-6\n",
            "):\n",
            "    return MambaInnerFn.apply(xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
            "                              out_proj_weight, out_proj_bias,\n",
            "                              A, B, C, D, delta_bias, B_proj_bias, C_proj_bias, delta_softplus, checkpoint_lvl,\n",
            "                              b_rms_weight, c_rms_weight, dt_rms_weight, b_c_dt_rms_eps)\n",
            "\n",
            "\n",
            "def mamba_inner_ref(\n",
            "        xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
            "        out_proj_weight, out_proj_bias,\n",
            "        A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
            "        C_proj_bias=None, delta_softplus=True\n",
            "):\n",
            "    assert causal_conv1d_fn is not None, \"causal_conv1d_fn is not available. Please install causal-conv1d.\"\n",
            "    L = xz.shape[-1]\n",
            "    delta_rank = delta_proj_weight.shape[1]\n",
            "    d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
            "    x, z = xz.chunk(2, dim=1)\n",
            "    x = causal_conv1d_fn(x, rearrange(conv1d_weight, \"d 1 w -> d w\"), conv1d_bias, activation=\"silu\")\n",
            "    # We're being very careful here about the layout, to avoid extra transposes.\n",
            "    # We want delta to have d as the slowest moving dimension\n",
            "    # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
            "    x_dbl = F.linear(rearrange(x, 'b d l -> (b l) d'), x_proj_weight)  # (bl d)\n",
            "    delta = delta_proj_weight @ x_dbl[:, :delta_rank].t()\n",
            "    delta = rearrange(delta, \"d (b l) -> b d l\", l=L)\n",
            "    if B is None:  # variable B\n",
            "        B = x_dbl[:, delta_rank:delta_rank + d_state]  # (bl d)\n",
            "        if B_proj_bias is not None:\n",
            "            B = B + B_proj_bias.to(dtype=B.dtype)\n",
            "        if not A.is_complex():\n",
            "            B = rearrange(B, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
            "        else:\n",
            "            B = rearrange(B, \"(b l) (dstate two) -> b dstate (l two)\", l=L, two=2).contiguous()\n",
            "    if C is None:  # variable B\n",
            "        C = x_dbl[:, -d_state:]  # (bl d)\n",
            "        if C_proj_bias is not None:\n",
            "            C = C + C_proj_bias.to(dtype=C.dtype)\n",
            "        if not A.is_complex():\n",
            "            C = rearrange(C, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
            "        else:\n",
            "            C = rearrange(C, \"(b l) (dstate two) -> b dstate (l two)\", l=L, two=2).contiguous()\n",
            "    y = selective_scan_fn(x, delta, A, B, C, D, z=z, delta_bias=delta_bias, delta_softplus=True)\n",
            "    return F.linear(rearrange(y, \"b d l -> b l d\"), out_proj_weight, out_proj_bias)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/mamba')\n",
        "from mamba_ssm import Mamba"
      ],
      "metadata": {
        "id": "ccypPrtcwYMj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model = Mamba(d_model=16, d_state=16, d_conv=4, expand=2).to(\"cuda\")\n",
        "x = torch.randn(1, 121, 16).to(\"cuda\")\n",
        "y = model(x)\n",
        "print(y.shape)  # Mong đợi: torch.Size([1, 121, 16])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4GHplojwZCd",
        "outputId": "96eeb258-c043-4602-9362-c79bd7fc7832"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 121, 16])\n"
          ]
        }
      ]
    }
  ]
}