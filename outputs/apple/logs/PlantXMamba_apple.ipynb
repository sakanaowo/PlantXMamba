{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torchvision.models as models\n",
        "from torchvision.models import VGG16_Weights\n",
        "\n",
        "\n",
        "def npo2(len):\n",
        "    \"\"\"\n",
        "    Returns the next power of 2 above len\n",
        "    \"\"\"\n",
        "\n",
        "    return 2 ** math.ceil(math.log2(len))\n",
        "\n",
        "\n",
        "def pad_npo2(X):\n",
        "    \"\"\"\n",
        "    Pads input length dim to the next power of 2\n",
        "\n",
        "    Args:\n",
        "        X : (B, L, D, N)\n",
        "\n",
        "    Returns:\n",
        "        Y : (B, npo2(L), D, N)\n",
        "    \"\"\"\n",
        "\n",
        "    len_npo2 = npo2(X.size(1))\n",
        "    pad_tuple = (0, 0, 0, 0, 0, len_npo2 - X.size(1))\n",
        "    return F.pad(X, pad_tuple, \"constant\", 0)\n",
        "\n",
        "\n",
        "class PScan(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def pscan(A, X):\n",
        "        # A : (B, D, L, N)\n",
        "        # X : (B, D, L, N)\n",
        "\n",
        "        # modifies X in place by doing a parallel scan.\n",
        "        # more formally, X will be populated by these values :\n",
        "        # H[t] = A[t] * H[t-1] + X[t] with H[0] = 0\n",
        "        # which are computed in parallel (2*log2(T) sequential steps (ideally), instead of T sequential steps)\n",
        "\n",
        "        # only supports L that is a power of two (mainly for a clearer code)\n",
        "\n",
        "        B, D, L, _ = A.size()\n",
        "        num_steps = int(math.log2(L))\n",
        "\n",
        "        # up sweep (last 2 steps unfolded)\n",
        "        Aa = A\n",
        "        Xa = X\n",
        "        for _ in range(num_steps - 2):\n",
        "            T = Xa.size(2)\n",
        "            Aa = Aa.view(B, D, T // 2, 2, -1)\n",
        "            Xa = Xa.view(B, D, T // 2, 2, -1)\n",
        "\n",
        "            Xa[:, :, :, 1].add_(Aa[:, :, :, 1].mul(Xa[:, :, :, 0]))\n",
        "            Aa[:, :, :, 1].mul_(Aa[:, :, :, 0])\n",
        "\n",
        "            Aa = Aa[:, :, :, 1]\n",
        "            Xa = Xa[:, :, :, 1]\n",
        "\n",
        "        # we have only 4, 2 or 1 nodes left\n",
        "        if Xa.size(2) == 4:\n",
        "            Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 0]))\n",
        "            Aa[:, :, 1].mul_(Aa[:, :, 0])\n",
        "\n",
        "            Xa[:, :, 3].add_(\n",
        "                Aa[:, :, 3].mul(Xa[:, :, 2] + Aa[:, :, 2].mul(Xa[:, :, 1]))\n",
        "            )\n",
        "        elif Xa.size(2) == 2:\n",
        "            Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 0]))\n",
        "            return\n",
        "        else:\n",
        "            return\n",
        "\n",
        "        # down sweep (first 2 steps unfolded)\n",
        "        Aa = A[:, :, 2 ** (num_steps - 2) - 1 : L : 2 ** (num_steps - 2)]\n",
        "        Xa = X[:, :, 2 ** (num_steps - 2) - 1 : L : 2 ** (num_steps - 2)]\n",
        "        Xa[:, :, 2].add_(Aa[:, :, 2].mul(Xa[:, :, 1]))\n",
        "        Aa[:, :, 2].mul_(Aa[:, :, 1])\n",
        "\n",
        "        for k in range(num_steps - 3, -1, -1):\n",
        "            Aa = A[:, :, 2**k - 1 : L : 2**k]\n",
        "            Xa = X[:, :, 2**k - 1 : L : 2**k]\n",
        "\n",
        "            T = Xa.size(2)\n",
        "            Aa = Aa.view(B, D, T // 2, 2, -1)\n",
        "            Xa = Xa.view(B, D, T // 2, 2, -1)\n",
        "\n",
        "            Xa[:, :, 1:, 0].add_(Aa[:, :, 1:, 0].mul(Xa[:, :, :-1, 1]))\n",
        "            Aa[:, :, 1:, 0].mul_(Aa[:, :, :-1, 1])\n",
        "\n",
        "    @staticmethod\n",
        "    def pscan_rev(A, X):\n",
        "        # A : (B, D, L, N)\n",
        "        # X : (B, D, L, N)\n",
        "\n",
        "        # the same function as above, but in reverse\n",
        "        # (if you flip the input, call pscan, then flip the output, you get what this function outputs)\n",
        "        # it is used in the backward pass\n",
        "\n",
        "        # only supports L that is a power of two (mainly for a clearer code)\n",
        "\n",
        "        B, D, L, _ = A.size()\n",
        "        num_steps = int(math.log2(L))\n",
        "\n",
        "        # up sweep (last 2 steps unfolded)\n",
        "        Aa = A\n",
        "        Xa = X\n",
        "        for _ in range(num_steps - 2):\n",
        "            T = Xa.size(2)\n",
        "            Aa = Aa.view(B, D, T // 2, 2, -1)\n",
        "            Xa = Xa.view(B, D, T // 2, 2, -1)\n",
        "\n",
        "            Xa[:, :, :, 0].add_(Aa[:, :, :, 0].mul(Xa[:, :, :, 1]))\n",
        "            Aa[:, :, :, 0].mul_(Aa[:, :, :, 1])\n",
        "\n",
        "            Aa = Aa[:, :, :, 0]\n",
        "            Xa = Xa[:, :, :, 0]\n",
        "\n",
        "        # we have only 4, 2 or 1 nodes left\n",
        "        if Xa.size(2) == 4:\n",
        "            Xa[:, :, 2].add_(Aa[:, :, 2].mul(Xa[:, :, 3]))\n",
        "            Aa[:, :, 2].mul_(Aa[:, :, 3])\n",
        "\n",
        "            Xa[:, :, 0].add_(\n",
        "                Aa[:, :, 0].mul(Xa[:, :, 1].add(Aa[:, :, 1].mul(Xa[:, :, 2])))\n",
        "            )\n",
        "        elif Xa.size(2) == 2:\n",
        "            Xa[:, :, 0].add_(Aa[:, :, 0].mul(Xa[:, :, 1]))\n",
        "            return\n",
        "        else:\n",
        "            return\n",
        "\n",
        "        # down sweep (first 2 steps unfolded)\n",
        "        Aa = A[:, :, 0 : L : 2 ** (num_steps - 2)]\n",
        "        Xa = X[:, :, 0 : L : 2 ** (num_steps - 2)]\n",
        "        Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 2]))\n",
        "        Aa[:, :, 1].mul_(Aa[:, :, 2])\n",
        "\n",
        "        for k in range(num_steps - 3, -1, -1):\n",
        "            Aa = A[:, :, 0 : L : 2**k]\n",
        "            Xa = X[:, :, 0 : L : 2**k]\n",
        "\n",
        "            T = Xa.size(2)\n",
        "            Aa = Aa.view(B, D, T // 2, 2, -1)\n",
        "            Xa = Xa.view(B, D, T // 2, 2, -1)\n",
        "\n",
        "            Xa[:, :, :-1, 1].add_(Aa[:, :, :-1, 1].mul(Xa[:, :, 1:, 0]))\n",
        "            Aa[:, :, :-1, 1].mul_(Aa[:, :, 1:, 0])\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, A_in, X_in):\n",
        "        \"\"\"\n",
        "        Applies the parallel scan operation, as defined above. Returns a new tensor.\n",
        "        If you can, privilege sequence lengths that are powers of two.\n",
        "\n",
        "        Args:\n",
        "            A_in : (B, L, D, N)\n",
        "            X_in : (B, L, D, N)\n",
        "\n",
        "        Returns:\n",
        "            H : (B, L, D, N)\n",
        "        \"\"\"\n",
        "\n",
        "        L = X_in.size(1)\n",
        "\n",
        "        # cloning is requiered because of the in-place ops\n",
        "        if L == npo2(L):\n",
        "            A = A_in.clone()\n",
        "            X = X_in.clone()\n",
        "        else:\n",
        "            # pad tensors (and clone btw)\n",
        "            A = pad_npo2(A_in)  # (B, npo2(L), D, N)\n",
        "            X = pad_npo2(X_in)  # (B, npo2(L), D, N)\n",
        "\n",
        "        # prepare tensors\n",
        "        A = A.transpose(2, 1)  # (B, D, npo2(L), N)\n",
        "        X = X.transpose(2, 1)  # (B, D, npo2(L), N)\n",
        "\n",
        "        # parallel scan (modifies X in-place)\n",
        "        PScan.pscan(A, X)\n",
        "\n",
        "        ctx.save_for_backward(A_in, X)\n",
        "\n",
        "        # slice [:, :L] (cut if there was padding)\n",
        "        return X.transpose(2, 1)[:, :L]\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output_in):\n",
        "        \"\"\"\n",
        "        Flows the gradient from the output to the input. Returns two new tensors.\n",
        "\n",
        "        Args:\n",
        "            ctx : A_in : (B, L, D, N), X : (B, D, L, N)\n",
        "            grad_output_in : (B, L, D, N)\n",
        "\n",
        "        Returns:\n",
        "            gradA : (B, L, D, N), gradX : (B, L, D, N)\n",
        "        \"\"\"\n",
        "\n",
        "        A_in, X = ctx.saved_tensors\n",
        "\n",
        "        L = grad_output_in.size(1)\n",
        "\n",
        "        # cloning is requiered because of the in-place ops\n",
        "        if L == npo2(L):\n",
        "            grad_output = grad_output_in.clone()\n",
        "            # the next padding will clone A_in\n",
        "        else:\n",
        "            grad_output = pad_npo2(grad_output_in)  # (B, npo2(L), D, N)\n",
        "            A_in = pad_npo2(A_in)  # (B, npo2(L), D, N)\n",
        "\n",
        "        # prepare tensors\n",
        "        grad_output = grad_output.transpose(2, 1)\n",
        "        A_in = A_in.transpose(2, 1)  # (B, D, npo2(L), N)\n",
        "        A = torch.nn.functional.pad(\n",
        "            A_in[:, :, 1:], (0, 0, 0, 1)\n",
        "        )  # (B, D, npo2(L), N) shift 1 to the left (see hand derivation)\n",
        "\n",
        "        # reverse parallel scan (modifies grad_output in-place)\n",
        "        PScan.pscan_rev(A, grad_output)\n",
        "\n",
        "        Q = torch.zeros_like(X)\n",
        "        Q[:, :, 1:].add_(X[:, :, :-1] * grad_output[:, :, 1:])\n",
        "\n",
        "        return Q.transpose(2, 1)[:, :L], grad_output.transpose(2, 1)[:, :L]\n",
        "\n",
        "\n",
        "pscan = PScan.apply"
      ],
      "metadata": {
        "id": "YmiT67Lp6v7N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Union\n",
        "\n",
        "@dataclass\n",
        "class MambaConfig:\n",
        "    d_model: int  # D\n",
        "    n_layers: int\n",
        "    dt_rank: Union[int, str] = \"auto\"\n",
        "    d_state: int = 16  # N in paper/comments\n",
        "    expand_factor: int = 2  # E in paper/comments\n",
        "    d_conv: int = 4\n",
        "\n",
        "    dt_min: float = 0.001\n",
        "    dt_max: float = 0.1\n",
        "    dt_init: str = \"random\"  # \"random\" or \"constant\"\n",
        "    dt_scale: float = 1.0\n",
        "    dt_init_floor = 1e-4\n",
        "\n",
        "    rms_norm_eps: float = 1e-5\n",
        "    base_std: float = 0.02\n",
        "\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    bias: bool = False\n",
        "    conv_bias: bool = True\n",
        "    inner_layernorms: bool = False  # apply layernorms to internal activations\n",
        "\n",
        "    mup: bool = False\n",
        "    mup_base_width: float = 128  # width=d_model\n",
        "\n",
        "    pscan: bool = True  # use parallel scan mode or sequential mode when training\n",
        "    use_cuda: bool = False  # use official CUDA implementation when training (not compatible with (b)float16)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = self.expand_factor * self.d_model  # E*D = ED in comments\n",
        "\n",
        "        if self.dt_rank == \"auto\":\n",
        "            self.dt_rank = math.ceil(self.d_model / 16)\n",
        "\n",
        "        # muP\n",
        "        if self.mup:\n",
        "            self.mup_width_mult = self.d_model / self.mup_base_width\n",
        "\n",
        "\n",
        "class Mamba(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [ResidualBlock(config) for _ in range(config.n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def step(self, x, caches):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, caches[i] = layer.step(x, caches[i])\n",
        "\n",
        "        return x, caches\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mixer = MambaBlock(config)\n",
        "        self.norm = RMSNorm(config.d_model, config.rms_norm_eps, config.mup)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        output = self.mixer(self.norm(x)) + x\n",
        "        return output\n",
        "\n",
        "    def step(self, x, cache):\n",
        "        output, cache = self.mixer.step(self.norm(x), cache)\n",
        "        output = output + x\n",
        "        return output, cache\n",
        "\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        # projects block input from D to 2*ED (two branches)\n",
        "        self.in_proj = nn.Linear(config.d_model, 2 * config.d_inner, bias=config.bias)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_channels=config.d_inner,\n",
        "            out_channels=config.d_inner,\n",
        "            kernel_size=config.d_conv,\n",
        "            bias=config.conv_bias,\n",
        "            groups=config.d_inner,\n",
        "            padding=config.d_conv - 1,\n",
        "        )\n",
        "\n",
        "        # projects x to input-dependent delta, B, C\n",
        "        self.x_proj = nn.Linear(\n",
        "            config.d_inner, config.dt_rank + 2 * config.d_state, bias=False\n",
        "        )\n",
        "\n",
        "        # projects delta from dt_rank to d_inner\n",
        "        self.dt_proj = nn.Linear(config.dt_rank, config.d_inner, bias=True)\n",
        "\n",
        "        # dt initialization\n",
        "        # dt weights\n",
        "        dt_init_std = config.dt_rank**-0.5 * config.dt_scale\n",
        "        if config.dt_init == \"constant\":\n",
        "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
        "        elif config.dt_init == \"random\":\n",
        "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # delta bias\n",
        "        dt = torch.exp(\n",
        "            torch.rand(config.d_inner)\n",
        "            * (math.log(config.dt_max) - math.log(config.dt_min))\n",
        "            + math.log(config.dt_min)\n",
        "        ).clamp(min=config.dt_init_floor)\n",
        "        inv_dt = dt + torch.log(\n",
        "            -torch.expm1(-dt)\n",
        "        )  # inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        with torch.no_grad():\n",
        "            self.dt_proj.bias.copy_(inv_dt)\n",
        "\n",
        "        # S4D real initialization\n",
        "        A = torch.arange(1, config.d_state + 1, dtype=torch.float32).repeat(\n",
        "            config.d_inner, 1\n",
        "        )\n",
        "        self.A_log = nn.Parameter(\n",
        "            torch.log(A)\n",
        "        )  # why store A in log ? to keep A < 0 (cf -torch.exp(...)) ? for gradient stability ?\n",
        "        self.A_log._no_weight_decay = True\n",
        "\n",
        "        self.D = nn.Parameter(torch.ones(config.d_inner))\n",
        "        self.D._no_weight_decay = True\n",
        "\n",
        "        # projects block output from ED back to D\n",
        "        self.out_proj = nn.Linear(config.d_inner, config.d_model, bias=config.bias)\n",
        "\n",
        "        # used in jamba\n",
        "        if self.config.inner_layernorms:\n",
        "            self.dt_layernorm = RMSNorm(\n",
        "                self.config.dt_rank, config.rms_norm_eps, config.mup\n",
        "            )\n",
        "            self.B_layernorm = RMSNorm(\n",
        "                self.config.d_state, config.rms_norm_eps, config.mup\n",
        "            )\n",
        "            self.C_layernorm = RMSNorm(\n",
        "                self.config.d_state, config.rms_norm_eps, config.mup\n",
        "            )\n",
        "        else:\n",
        "            self.dt_layernorm = None\n",
        "            self.B_layernorm = None\n",
        "            self.C_layernorm = None\n",
        "\n",
        "        if self.config.use_cuda:\n",
        "            try:\n",
        "                from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
        "\n",
        "                self.selective_scan_cuda = selective_scan_fn\n",
        "            except ImportError:\n",
        "                print(\"Failed to import mamba_ssm. Falling back to mamba.py.\")\n",
        "                self.config.use_cuda = False\n",
        "\n",
        "    def _apply_layernorms(self, dt, B, C):\n",
        "        if self.dt_layernorm is not None:\n",
        "            dt = self.dt_layernorm(dt)\n",
        "        if self.B_layernorm is not None:\n",
        "            B = self.B_layernorm(B)\n",
        "        if self.C_layernorm is not None:\n",
        "            C = self.C_layernorm(C)\n",
        "        return dt, B, C\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        _, L, _ = x.shape\n",
        "\n",
        "        xz = self.in_proj(x)  # (B, L, 2*ED)\n",
        "        x, z = xz.chunk(2, dim=-1)  # (B, L, ED), (B, L, ED)\n",
        "\n",
        "        # x branch\n",
        "        x = x.transpose(1, 2)  # (B, ED, L)\n",
        "        x = self.conv1d(x)[\n",
        "            :, :, :L\n",
        "        ]  # depthwise convolution over time, with a short filter\n",
        "        x = x.transpose(1, 2)  # (B, L, ED)\n",
        "\n",
        "        x = F.silu(x)\n",
        "        y = self.ssm(x, z)\n",
        "\n",
        "        if self.config.use_cuda:\n",
        "            output = self.out_proj(y)  # (B, L, D)\n",
        "            return output  # the rest of the operations are done in the ssm function (fused with the CUDA pscan)\n",
        "\n",
        "        # z branch\n",
        "        z = F.silu(z)\n",
        "\n",
        "        output = y * z\n",
        "        output = self.out_proj(output)  # (B, L, D)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def ssm(self, x, z):\n",
        "\n",
        "        A = -torch.exp(self.A_log.float())  # (ED, N)\n",
        "        D = self.D.float()\n",
        "\n",
        "        deltaBC = self.x_proj(x)  # (B, L, dt_rank+2*N)\n",
        "        delta, B, C = torch.split(\n",
        "            deltaBC,\n",
        "            [self.config.dt_rank, self.config.d_state, self.config.d_state],\n",
        "            dim=-1,\n",
        "        )  # (B, L, dt_rank), (B, L, N), (B, L, N)\n",
        "        delta, B, C = self._apply_layernorms(delta, B, C)\n",
        "        delta = self.dt_proj.weight @ delta.transpose(\n",
        "            1, 2\n",
        "        )  # (ED, dt_rank) @ (B, L, dt_rank) -> (B, ED, L)\n",
        "\n",
        "        if self.config.use_cuda:\n",
        "            # these are unfortunately needed for the selective_scan_cuda function\n",
        "            x = x.transpose(1, 2)\n",
        "            B = B.transpose(1, 2)\n",
        "            C = C.transpose(1, 2)\n",
        "            z = z.transpose(1, 2)\n",
        "\n",
        "            # \"softplus\" + \"bias\" + \"y * silu(z)\" operations are fused\n",
        "            y = self.selective_scan_cuda(\n",
        "                x,\n",
        "                delta,\n",
        "                A,\n",
        "                B,\n",
        "                C,\n",
        "                D,\n",
        "                z=z,\n",
        "                delta_softplus=True,\n",
        "                delta_bias=self.dt_proj.bias.float(),\n",
        "            )\n",
        "            y = y.transpose(1, 2)  # (B, L, ED)\n",
        "\n",
        "        else:\n",
        "            delta = delta.transpose(1, 2)\n",
        "            delta = F.softplus(delta + self.dt_proj.bias)\n",
        "\n",
        "            if self.config.pscan:\n",
        "                y = self.selective_scan(x, delta, A, B, C, D)\n",
        "            else:\n",
        "                y = self.selective_scan_seq(x, delta, A, B, C, D)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def selective_scan(self, x, delta, A, B, C, D):\n",
        "\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, L, ED, N)\n",
        "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  # (B, L, ED, N)\n",
        "\n",
        "        BX = deltaB * (x.unsqueeze(-1))  # (B, L, ED, N)\n",
        "\n",
        "        hs = pscan(deltaA, BX)\n",
        "\n",
        "        y = (hs @ C.unsqueeze(-1)).squeeze(\n",
        "            3\n",
        "        )  # (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n",
        "\n",
        "        y = y + D * x\n",
        "\n",
        "        return y\n",
        "\n",
        "    def selective_scan_seq(self, x, delta, A, B, C, D):\n",
        "        _, L, _ = x.shape\n",
        "\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, L, ED, N)\n",
        "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  # (B, L, ED, N)\n",
        "\n",
        "        BX = deltaB * (x.unsqueeze(-1))  # (B, L, ED, N)\n",
        "\n",
        "        h = torch.zeros(\n",
        "            x.size(0), self.config.d_inner, self.config.d_state, device=deltaA.device\n",
        "        )  # (B, ED, N)\n",
        "        hs = []\n",
        "\n",
        "        for t in range(0, L):\n",
        "            h = deltaA[:, t] * h + BX[:, t]\n",
        "            hs.append(h)\n",
        "\n",
        "        hs = torch.stack(hs, dim=1)  # (B, L, ED, N)\n",
        "\n",
        "        y = (hs @ C.unsqueeze(-1)).squeeze(\n",
        "            3\n",
        "        )  # (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n",
        "\n",
        "        y = y + D * x\n",
        "\n",
        "        return y\n",
        "\n",
        "    # -------------------------- inference -------------------------- #\n",
        "\n",
        "    def step(self, x, cache):\n",
        "        # x : (B, D)\n",
        "        # cache : (h, inputs)\n",
        "        # h : (B, ED, N)\n",
        "        # inputs : (B, ED, d_conv-1)\n",
        "\n",
        "        # y : (B, D)\n",
        "        # cache : (h, inputs)\n",
        "\n",
        "        h, inputs = cache\n",
        "\n",
        "        xz = self.in_proj(x)  # (B, 2*ED)\n",
        "        x, z = xz.chunk(2, dim=1)  # (B, ED), (B, ED)\n",
        "\n",
        "        # x branch\n",
        "        x_cache = x.unsqueeze(2)\n",
        "        x = self.conv1d(torch.cat([inputs, x_cache], dim=2))[\n",
        "            :, :, self.config.d_conv - 1\n",
        "        ]  # (B, ED)\n",
        "\n",
        "        x = F.silu(x)\n",
        "        y, h = self.ssm_step(x, h)\n",
        "\n",
        "        # z branch\n",
        "        z = F.silu(z)\n",
        "\n",
        "        output = y * z\n",
        "        output = self.out_proj(output)  # (B, D)\n",
        "\n",
        "        # prepare cache for next call\n",
        "        inputs = torch.cat([inputs[:, :, 1:], x_cache], dim=2)  # (B, ED, d_conv-1)\n",
        "        cache = (h, inputs)\n",
        "\n",
        "        return output, cache\n",
        "\n",
        "    def ssm_step(self, x, h):\n",
        "        # x : (B, ED)\n",
        "        # h : (B, ED, N)\n",
        "\n",
        "        # y : (B, ED)\n",
        "        # h : (B, ED, N)\n",
        "\n",
        "        A = -torch.exp(\n",
        "            self.A_log.float()\n",
        "        )  # (ED, N) # todo : ne pas le faire tout le temps, puisque c'est indépendant de la timestep\n",
        "        D = self.D.float()\n",
        "\n",
        "        deltaBC = self.x_proj(x)  # (B, dt_rank+2*N)\n",
        "\n",
        "        delta, B, C = torch.split(\n",
        "            deltaBC,\n",
        "            [self.config.dt_rank, self.config.d_state, self.config.d_state],\n",
        "            dim=-1,\n",
        "        )  # (B, dt_rank), (B, N), (B, N)\n",
        "        delta, B, C = self._apply_layernorms(delta, B, C)\n",
        "        delta = F.softplus(self.dt_proj(delta))  # (B, ED)\n",
        "\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, ED, N)\n",
        "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(1)  # (B, ED, N)\n",
        "\n",
        "        BX = deltaB * (x.unsqueeze(-1))  # (B, ED, N)\n",
        "\n",
        "        if h is None:\n",
        "            h = torch.zeros(\n",
        "                x.size(0),\n",
        "                self.config.d_inner,\n",
        "                self.config.d_state,\n",
        "                device=deltaA.device,\n",
        "            )  # (B, ED, N)\n",
        "\n",
        "        h = deltaA * h + BX  # (B, ED, N)\n",
        "\n",
        "        y = (h @ C.unsqueeze(-1)).squeeze(2)  # (B, ED, N) @ (B, N, 1) -> (B, ED, 1)\n",
        "\n",
        "        y = y + D * x\n",
        "\n",
        "        return y, h\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, d_model: int, eps: float = 1e-5, use_mup: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_mup = use_mup\n",
        "        self.eps = eps\n",
        "\n",
        "        # https://arxiv.org/abs/2404.05728, RMSNorm gains prevents muTransfer (section 4.2.3)\n",
        "        if not use_mup:\n",
        "            self.weight = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "        if not self.use_mup:\n",
        "            return output * self.weight\n",
        "        else:\n",
        "            return output"
      ],
      "metadata": {
        "id": "YabPZAm767UR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MambaHead(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.norm(x)\n",
        "        x = self.dropout(x)\n",
        "        return x  # (batch_size, seq_len, d_model)\n"
      ],
      "metadata": {
        "id": "mo-pfKiC688_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MambaModule(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.d_model = self.args.d_model\n",
        "        self.n_layers = self.args.n_layers\n",
        "\n",
        "        config = MambaConfig(d_model=self.d_model, n_layers=self.n_layers,\n",
        "                           d_state=self.args.d_state, d_conv=self.args.d_conv,\n",
        "                           expand_factor=self.args.expand,dropout=self.args.dropout)\n",
        "        self.backbone = Mamba(config)\n",
        "        self.head = MambaHead(d_model=self.d_model, dropout=self.args.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        sequence_output = self.backbone(x)  # (batch_size, seq_len, d_model)\n",
        "        output = self.head(sequence_output)  # (batch_size, seq_len, d_model)\n",
        "        return output"
      ],
      "metadata": {
        "id": "brjDFfkq6-3E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sakanaowo/PlantXViT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEvCbz8Q7E2G",
        "outputId": "7806b261-facc-4b30-e999-ab335da21b1a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PlantXViT'...\n",
            "remote: Enumerating objects: 104825, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 104825 (delta 6), reused 20 (delta 4), pack-reused 104802 (from 1)\u001b[K\n",
            "Receiving objects: 100% (104825/104825), 2.45 GiB | 58.12 MiB/s, done.\n",
            "Resolving deltas: 100% (30447/30447), done.\n",
            "Updating files: 100% (104353/104353), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PlantXViT.utils.config_loader import load_config\n",
        "config=load_config('PlantXViT/configs/config.yaml')"
      ],
      "metadata": {
        "id": "-itPSeZO7GW3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import pickle"
      ],
      "metadata": {
        "id": "K5j4B2SA7Hta"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apple_config = config['dataset']['apple']\n",
        "img_dir = apple_config['data_dir']\n",
        "csv_path='./PlantXViT/data/raw/plant-pathology-2020-fgvc7/train.csv'\n",
        "label_encoder_path='./PlantXViT/data/processed/apple_label_encoder.pkl'\n",
        "image_size = tuple(apple_config['image_size'])"
      ],
      "metadata": {
        "id": "Fq6ub0z67JRP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_path)\n",
        "df['label'] = df[['healthy', 'multiple_diseases', 'rust', 'scab']].idxmax(axis=1)\n",
        "\n",
        "# encode label\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# split train/val\n",
        "train_df, val_df = train_test_split(df, test_size=0.2,\n",
        "                                    stratify=df['label'],\n",
        "                                    random_state=42)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['label_idx'] = label_encoder.fit_transform(train_df['label'])\n",
        "val_df['label_idx'] = label_encoder.transform(val_df['label'])\n",
        "\n",
        "image_dir = \"./PlantXViT/data/raw/plant-pathology-2020-fgvc7/images\"\n",
        "train_df = pd.read_csv(\"./PlantXViT/data/processed/apple/apple_train.csv\")\n",
        "val_df = pd.read_csv(\"./PlantXViT/data/processed/apple/apple_val.csv\")"
      ],
      "metadata": {
        "id": "o3eQGQVi7OKU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class AppleDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transform):\n",
        "        self.df = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.image_dir, row['image_id'] + \".jpg\")\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "        label = torch.tensor(row['label_idx'])\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "J5HXJ7BO7RnK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = config[\"training\"][\"batch_size\"]\n",
        "\n",
        "train_dataset = AppleDataset(train_df, img_dir, transform)\n",
        "val_dataset = AppleDataset(val_df, img_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "r7ZXab117TY8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels=128):\n",
        "        super(InceptionBlock, self).__init__()\n",
        "        # Nhánh 1: 1x1\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 128, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128)\n",
        "        )\n",
        "\n",
        "        # Nhánh 2: 1x1 -> 3x1 + 1x3\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 96, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.Conv2d(96, 128, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128)\n",
        "        )\n",
        "\n",
        "        # Nhánh 3: 1x1 -> 3x1 + 1x3 -> 3x1 + 1x3\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 96, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.Conv2d(96, 96, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.Conv2d(96, 192, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.Conv2d(192, 192, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(192)\n",
        "        )\n",
        "\n",
        "        # Nhánh 4: MaxPool -> 1x1\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.branch1x1(x)\n",
        "        b2 = self.branch3x3(x)\n",
        "        b3 = self.branch5x5(x)\n",
        "        b4 = self.branch_pool(x)\n",
        "        return torch.cat([b1, b2, b3, b4], dim=1)\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size=5, emb_size=16):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.emb_size = emb_size\n",
        "        self.proj = nn.Linear(in_channels * patch_size * patch_size, emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
        "        x = x.view(B, -1, C * self.patch_size * self.patch_size)\n",
        "        return self.proj(x)  # shape: (b,num patches,emb size)\n",
        "\n",
        "\n",
        "class PlantXMamba(nn.Module):\n",
        "    def __init__(self, num_classes=4, patch_size=5, emb_size=16, d_state=64,d_conv=64,expand=4,n_layers=2,num_blocks=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # VGG16 (2 blocks)\n",
        "        vgg = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
        "        self.vgg_block = nn.Sequential(*list(vgg.features[:10]))\n",
        "\n",
        "        # Inception-like block → (B, 512, 56, 56)\n",
        "        self.inception = InceptionBlock(in_channels=128)\n",
        "\n",
        "        # Patch Embedding → (B, 121, 16)\n",
        "        self.patch_embed = PatchEmbedding(in_channels=512, patch_size=patch_size, emb_size=emb_size)\n",
        "\n",
        "        # Mamba blocks\n",
        "        mamba_args = type('Args', (), {\n",
        "            'd_model': emb_size,\n",
        "            'd_state': d_state,\n",
        "            'd_conv': d_conv,\n",
        "            'expand': expand,\n",
        "            'n_layers': n_layers,\n",
        "            'dropout': dropout\n",
        "        })()\n",
        "        self.mamba = nn.Sequential(*[MambaModule(mamba_args) for _ in range(num_blocks)])\n",
        "\n",
        "        # Classification head\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vgg_block(x)  # (B, 128, 56, 56)\n",
        "        x = self.inception(x)  # (B, 512, 56, 56)\n",
        "        x = self.patch_embed(x)  # (B, 121, 16)\n",
        "        x = self.mamba(x)  # (B, 121, 16)\n",
        "        x = self.norm(x)  # (B, 121, 16)\n",
        "        x = x.permute(0, 2, 1)  # (B, 16, 121)\n",
        "        x = self.global_pool(x).squeeze(-1)  # (B, 16)\n",
        "        return self.classifier(x)  # (B, num_classes)"
      ],
      "metadata": {
        "id": "Lnbi3E7j7Yp4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PlantXViT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRfp7hQg8TxX",
        "outputId": "51a8a5f2-a121-4836-f59b-22fdd36d2bf8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PlantXViT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "    return epoch_loss, accuracy\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "    return epoch_loss, accuracy\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = PlantXMamba(num_classes=4).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def train_model(\n",
        "    model, train_loader, val_loader,\n",
        "    criterion, optimizer,\n",
        "    num_epochs, device,\n",
        "    save_path=\"./outputs/models/plantxvit_best.pth\"\n",
        "):\n",
        "    best_val_loss = float('inf')\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch [{epoch}/{num_epochs}] \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(\"✅ Saved best model.\")\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "ZXtVE7LV7Xd8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=50,\n",
        "    device=device,\n",
        "    save_path=\"./outputs/apple/models/PlantXMamba_apple_296.pth\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "OwWGYh0h7ait",
        "outputId": "a82f20b7-6d25-436c-fcbf-1f768e24f26f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Train Loss: 1.0908, Acc: 0.5707 | Val Loss: 0.9010, Acc: 0.7178\n",
            "✅ Saved best model.\n",
            "Epoch [2/50] Train Loss: 0.9154, Acc: 0.7163 | Val Loss: 0.8528, Acc: 0.7178\n",
            "✅ Saved best model.\n",
            "Epoch [3/50] Train Loss: 0.7902, Acc: 0.7562 | Val Loss: 0.7288, Acc: 0.7534\n",
            "✅ Saved best model.\n",
            "Epoch [4/50] Train Loss: 0.6616, Acc: 0.7967 | Val Loss: 0.6600, Acc: 0.7808\n",
            "✅ Saved best model.\n",
            "Epoch [5/50] Train Loss: 0.5603, Acc: 0.8413 | Val Loss: 0.5806, Acc: 0.8219\n",
            "✅ Saved best model.\n",
            "Epoch [6/50] Train Loss: 0.4429, Acc: 0.8956 | Val Loss: 0.6012, Acc: 0.8137\n",
            "Epoch [7/50] Train Loss: 0.3809, Acc: 0.9155 | Val Loss: 0.6281, Acc: 0.7973\n",
            "Epoch [8/50] Train Loss: 0.3274, Acc: 0.9293 | Val Loss: 0.6202, Acc: 0.8137\n",
            "Epoch [9/50] Train Loss: 0.3012, Acc: 0.9334 | Val Loss: 0.7159, Acc: 0.7342\n",
            "Epoch [10/50] Train Loss: 0.2525, Acc: 0.9409 | Val Loss: 0.5716, Acc: 0.8110\n",
            "✅ Saved best model.\n",
            "Epoch [11/50] Train Loss: 0.2440, Acc: 0.9382 | Val Loss: 0.5755, Acc: 0.8137\n",
            "Epoch [12/50] Train Loss: 0.1845, Acc: 0.9560 | Val Loss: 0.5699, Acc: 0.8301\n",
            "✅ Saved best model.\n",
            "Epoch [13/50] Train Loss: 0.1413, Acc: 0.9794 | Val Loss: 0.6754, Acc: 0.8000\n",
            "Epoch [14/50] Train Loss: 0.1770, Acc: 0.9663 | Val Loss: 0.9109, Acc: 0.7233\n",
            "Epoch [15/50] Train Loss: 0.1518, Acc: 0.9712 | Val Loss: 0.6780, Acc: 0.7945\n",
            "Epoch [16/50] Train Loss: 0.1270, Acc: 0.9815 | Val Loss: 0.7612, Acc: 0.7726\n",
            "Epoch [17/50] Train Loss: 0.1157, Acc: 0.9821 | Val Loss: 0.7872, Acc: 0.7534\n",
            "Epoch [18/50] Train Loss: 0.0814, Acc: 0.9952 | Val Loss: 0.6913, Acc: 0.8000\n",
            "Epoch [19/50] Train Loss: 0.0654, Acc: 0.9973 | Val Loss: 0.6578, Acc: 0.8027\n",
            "Epoch [20/50] Train Loss: 0.0518, Acc: 0.9986 | Val Loss: 0.7222, Acc: 0.7890\n",
            "Epoch [21/50] Train Loss: 0.0459, Acc: 0.9986 | Val Loss: 0.6813, Acc: 0.7945\n",
            "Epoch [22/50] Train Loss: 0.0376, Acc: 1.0000 | Val Loss: 0.7086, Acc: 0.7973\n",
            "Epoch [23/50] Train Loss: 0.0330, Acc: 1.0000 | Val Loss: 0.7069, Acc: 0.8055\n",
            "Epoch [24/50] Train Loss: 0.0299, Acc: 1.0000 | Val Loss: 0.7309, Acc: 0.7890\n",
            "Epoch [25/50] Train Loss: 0.0269, Acc: 1.0000 | Val Loss: 0.7402, Acc: 0.7973\n",
            "Epoch [26/50] Train Loss: 0.0244, Acc: 1.0000 | Val Loss: 0.7473, Acc: 0.8055\n",
            "Epoch [27/50] Train Loss: 0.0222, Acc: 1.0000 | Val Loss: 0.7720, Acc: 0.7973\n",
            "Epoch [28/50] Train Loss: 0.0205, Acc: 1.0000 | Val Loss: 0.7776, Acc: 0.7836\n",
            "Epoch [29/50] Train Loss: 0.0188, Acc: 1.0000 | Val Loss: 0.7935, Acc: 0.7863\n",
            "Epoch [30/50] Train Loss: 0.0176, Acc: 1.0000 | Val Loss: 0.8507, Acc: 0.7726\n",
            "Epoch [31/50] Train Loss: 0.0162, Acc: 1.0000 | Val Loss: 0.8333, Acc: 0.7808\n",
            "Epoch [32/50] Train Loss: 0.0149, Acc: 1.0000 | Val Loss: 0.8407, Acc: 0.7808\n",
            "Epoch [33/50] Train Loss: 0.0139, Acc: 1.0000 | Val Loss: 0.8453, Acc: 0.7808\n",
            "Epoch [34/50] Train Loss: 0.0130, Acc: 1.0000 | Val Loss: 0.8599, Acc: 0.7918\n",
            "Epoch [35/50] Train Loss: 0.0121, Acc: 1.0000 | Val Loss: 0.8678, Acc: 0.7753\n",
            "Epoch [36/50] Train Loss: 0.0112, Acc: 1.0000 | Val Loss: 0.8758, Acc: 0.7781\n",
            "Epoch [37/50] Train Loss: 0.0105, Acc: 1.0000 | Val Loss: 0.8917, Acc: 0.7808\n",
            "Epoch [38/50] Train Loss: 0.0098, Acc: 1.0000 | Val Loss: 0.9154, Acc: 0.7753\n",
            "Epoch [39/50] Train Loss: 0.0092, Acc: 1.0000 | Val Loss: 0.9271, Acc: 0.7726\n",
            "Epoch [40/50] Train Loss: 0.0086, Acc: 1.0000 | Val Loss: 0.9308, Acc: 0.7726\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-18-682771164.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = train_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-17-1553631484.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, save_path)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-17-1553631484.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrunning_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-10-558826353.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2314\u001b[0m                 )\n\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m     def reduce(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PlantXViT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsgGteO_F1uP",
        "outputId": "d3006648-ad8e-4f9d-813a-e9f4588b3b3e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PlantXViT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, accuracy_score\n",
        "from torch.nn.functional import softmax\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Định nghĩa các lớp và hàm từ log\n",
        "# ... (Copy các định nghĩa lớp: PScan, MambaConfig, Mamba, ResidualBlock, MambaBlock, RMSNorm, MambaHead, MambaModule, PatchEmbedding, VGGPlantXMamba từ log)\n",
        "\n",
        "# Định nghĩa AppleDataset\n",
        "class AppleDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transform):\n",
        "        self.df = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.image_dir, row['image_id'] + \".jpg\")\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "        label = torch.tensor(row['label_idx'])\n",
        "        return image, label\n",
        "\n",
        "# Khởi tạo mô hình\n",
        "model = PlantXMamba(num_classes=4)  # 4 lớp cho Apple dataset\n",
        "MODEL_PATH = \"./outputs/apple/models/PlantXMamba_apple_296.pth\"\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Tải trọng số mô hình\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# Chuẩn bị dữ liệu kiểm tra\n",
        "image_size = (224, 224)  # Từ log\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Tạo tập kiểm tra từ train.csv\n",
        "csv_path = \"./data/raw/plant-pathology-2020-fgvc7/train.csv\"\n",
        "image_dir = \"./data/raw/plant-pathology-2020-fgvc7/images\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Xử lý nhãn\n",
        "df['label'] = df[['healthy', 'multiple_diseases', 'rust', 'scab']].idxmax(axis=1)\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_idx'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Chia dữ liệu thành train, val, test\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
        "\n",
        "# Tạo dataset và dataloader cho tập kiểm tra\n",
        "test_dataset = AppleDataset(test_df, image_dir, transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Hàm đánh giá\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "total_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Lấy xác suất và nhãn dự đoán\n",
        "        probs = softmax(outputs, dim=1).cpu().numpy()  # Xác suất cho tất cả lớp\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()  # Nhãn dự đoán\n",
        "        labels = labels.cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels)\n",
        "        all_probs.extend(probs)\n",
        "\n",
        "# Chuyển sang numpy array\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "all_probs = np.array(all_probs)\n",
        "\n",
        "# Tính các chỉ số\n",
        "loss = total_loss / len(test_dataset)  # Mất mát trung bình\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "kappa = cohen_kappa_score(all_labels, all_preds)\n",
        "\n",
        "# AUC cho bài toán đa lớp (one-vs-rest)\n",
        "auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')\n",
        "\n",
        "# In kết quả\n",
        "print(f\"Loss (Mất mát): {loss:.4f}\")\n",
        "print(f\"Accuracy (Độ chính xác): {accuracy:.4f}\")\n",
        "print(f\"Precision (Độ chính xác dự đoán dương): {precision:.4f}\")\n",
        "print(f\"Recall (Tỷ lệ phát hiện dương): {recall:.4f}\")\n",
        "print(f\"F1 Score (Trung bình điều hòa): {f1:.4f}\")\n",
        "print(f\"AUC (Diện tích dưới đường cong ROC): {auc:.4f}\")\n",
        "print(f\"Kappa Score (Độ đo Cohen’s Kappa): {kappa:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wHqWyYoFp87",
        "outputId": "c86ad9b9-304d-447d-ea78-ae5c0268d934"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss (Mất mát): 0.3700\n",
            "Accuracy (Độ chính xác): 0.8942\n",
            "Precision (Độ chính xác dự đoán dương): 0.8862\n",
            "Recall (Tỷ lệ phát hiện dương): 0.8942\n",
            "F1 Score (Trung bình điều hòa): 0.8822\n",
            "AUC (Diện tích dưới đường cong ROC): 0.9715\n",
            "Kappa Score (Độ đo Cohen’s Kappa): 0.8452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FW5ZVAuxGjLo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}