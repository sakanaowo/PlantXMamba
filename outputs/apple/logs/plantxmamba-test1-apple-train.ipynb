{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/sakanaowo/PlantXMamba","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:09:06.763580Z","iopub.execute_input":"2025-06-22T14:09:06.764424Z","iopub.status.idle":"2025-06-22T14:09:07.512166Z","shell.execute_reply.started":"2025-06-22T14:09:06.764398Z","shell.execute_reply":"2025-06-22T14:09:07.511153Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'PlantXMamba'...\nremote: Enumerating objects: 106, done.\u001b[K\nremote: Counting objects: 100% (106/106), done.\u001b[K\nremote: Compressing objects: 100% (83/83), done.\u001b[K\nremote: Total 106 (delta 25), reused 96 (delta 15), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (106/106), 291.76 KiB | 6.79 MiB/s, done.\nResolving deltas: 100% (25/25), done.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!git clone https://github.com/sakanaowo/PlantXViT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:09:07.513789Z","iopub.execute_input":"2025-06-22T14:09:07.514092Z","iopub.status.idle":"2025-06-22T14:10:34.023975Z","shell.execute_reply.started":"2025-06-22T14:09:07.514054Z","shell.execute_reply":"2025-06-22T14:10:34.023196Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'PlantXViT'...\nremote: Enumerating objects: 104825, done.\u001b[K\nremote: Counting objects: 100% (23/23), done.\u001b[K\nremote: Compressing objects: 100% (17/17), done.\u001b[K\nremote: Total 104825 (delta 6), reused 20 (delta 4), pack-reused 104802 (from 1)\u001b[K\nReceiving objects: 100% (104825/104825), 2.45 GiB | 43.00 MiB/s, done.\nResolving deltas: 100% (30447/30447), done.\nUpdating files: 100% (104353/104353), done.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from PlantXViT.utils.config_loader import load_config\nconfig=load_config('PlantXViT/configs/config.yaml')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:10:34.025015Z","iopub.execute_input":"2025-06-22T14:10:34.025258Z","iopub.status.idle":"2025-06-22T14:10:34.033460Z","shell.execute_reply.started":"2025-06-22T14:10:34.025233Z","shell.execute_reply":"2025-06-22T14:10:34.032093Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:10:34.035466Z","iopub.execute_input":"2025-06-22T14:10:34.035656Z","iopub.status.idle":"2025-06-22T14:10:37.707839Z","shell.execute_reply.started":"2025-06-22T14:10:34.035641Z","shell.execute_reply":"2025-06-22T14:10:37.706784Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"Apple train attempt","metadata":{}},{"cell_type":"code","source":"apple_config = config['dataset']['apple']\nimg_dir = apple_config['data_dir']\ncsv_path = apple_config['csv_path']\nlabel_encoder_path = apple_config['label_encoder']\nimage_size = tuple(apple_config['image_size'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:10:37.708925Z","iopub.execute_input":"2025-06-22T14:10:37.709537Z","iopub.status.idle":"2025-06-22T14:10:37.728220Z","shell.execute_reply.started":"2025-06-22T14:10:37.709503Z","shell.execute_reply":"2025-06-22T14:10:37.727454Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"csv_path='./PlantXViT/data/raw/plant-pathology-2020-fgvc7/train.csv'\nlabel_encoder_path='./PlantXViT/data/processed/apple_label_encoder.pkl'\nprint(label_encoder_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:10:37.729166Z","iopub.execute_input":"2025-06-22T14:10:37.729581Z","iopub.status.idle":"2025-06-22T14:10:37.804984Z","shell.execute_reply.started":"2025-06-22T14:10:37.729556Z","shell.execute_reply":"2025-06-22T14:10:37.804089Z"}},"outputs":[{"name":"stdout","text":"./PlantXViT/data/processed/apple_label_encoder.pkl\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"df = pd.read_csv(csv_path)\ndf['label'] = df[['healthy', 'multiple_diseases', 'rust', 'scab']].idxmax(axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:10:37.806101Z","iopub.execute_input":"2025-06-22T14:10:37.806400Z","iopub.status.idle":"2025-06-22T14:10:37.828439Z","shell.execute_reply.started":"2025-06-22T14:10:37.806372Z","shell.execute_reply":"2025-06-22T14:10:37.827612Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# encode label\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:10:37.829633Z","iopub.execute_input":"2025-06-22T14:10:37.829928Z","iopub.status.idle":"2025-06-22T14:10:37.834547Z","shell.execute_reply.started":"2025-06-22T14:10:37.829909Z","shell.execute_reply":"2025-06-22T14:10:37.833746Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# split train/val\ntrain_df, val_df = train_test_split(df, test_size=0.2,\n                                    stratify=df['label'],\n                                    random_state=42)\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ntrain_df['label_idx'] = label_encoder.fit_transform(train_df['label'])\nval_df['label_idx'] = label_encoder.transform(val_df['label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:10:37.835302Z","iopub.execute_input":"2025-06-22T14:10:37.835582Z","iopub.status.idle":"2025-06-22T14:10:37.852522Z","shell.execute_reply.started":"2025-06-22T14:10:37.835558Z","shell.execute_reply":"2025-06-22T14:10:37.851874Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"image_dir = \"./PlantXViT/data/raw/plant-pathology-2020-fgvc7/images\"\ntrain_df = pd.read_csv(\"./PlantXViT/data/processed/apple/apple_train.csv\")\nval_df = pd.read_csv(\"./PlantXViT/data/processed/apple/apple_val.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:10:37.854624Z","iopub.execute_input":"2025-06-22T14:10:37.855080Z","iopub.status.idle":"2025-06-22T14:10:37.867445Z","shell.execute_reply.started":"2025-06-22T14:10:37.855050Z","shell.execute_reply":"2025-06-22T14:10:37.866593Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\nclass AppleDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transform):\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.image_dir, row['image_id'] + \".jpg\")\n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.transform(image)\n        label = torch.tensor(row['label_idx'])\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:10:37.868162Z","iopub.execute_input":"2025-06-22T14:10:37.868366Z","iopub.status.idle":"2025-06-22T14:10:37.875047Z","shell.execute_reply.started":"2025-06-22T14:10:37.868351Z","shell.execute_reply":"2025-06-22T14:10:37.874381Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = config[\"training\"][\"batch_size\"]\n\ntrain_dataset = AppleDataset(train_df, img_dir, transform)\nval_dataset = AppleDataset(val_df, img_dir, transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:10:37.875704Z","iopub.execute_input":"2025-06-22T14:10:37.875933Z","iopub.status.idle":"2025-06-22T14:10:37.890545Z","shell.execute_reply.started":"2025-06-22T14:10:37.875917Z","shell.execute_reply":"2025-06-22T14:10:37.889868Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"%cd PlantXMamba","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:10:37.891302Z","iopub.execute_input":"2025-06-22T14:10:37.891509Z","iopub.status.idle":"2025-06-22T14:10:37.908246Z","shell.execute_reply.started":"2025-06-22T14:10:37.891495Z","shell.execute_reply":"2025-06-22T14:10:37.907580Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/PlantXMamba/PlantXMamba\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"install mamba","metadata":{}},{"cell_type":"code","source":"!pip install -q -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:23:53.226006Z","iopub.execute_input":"2025-06-22T15:23:53.226761Z","iopub.status.idle":"2025-06-22T15:23:56.255708Z","shell.execute_reply.started":"2025-06-22T15:23:53.226729Z","shell.execute_reply":"2025-06-22T15:23:56.254915Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"!pip show causal-conv1d ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:26:14.276914Z","iopub.execute_input":"2025-06-22T15:26:14.277513Z","iopub.status.idle":"2025-06-22T15:26:16.210670Z","shell.execute_reply.started":"2025-06-22T15:26:14.277481Z","shell.execute_reply":"2025-06-22T15:26:16.209938Z"}},"outputs":[{"name":"stdout","text":"Name: causal-conv1d\nVersion: 1.5.0.post8\nSummary: Causal depthwise conv1d in CUDA, with a PyTorch interface\nHome-page: https://github.com/Dao-AILab/causal-conv1d\nAuthor: Tri Dao\nAuthor-email: tri@tridao.me\nLicense: \nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: ninja, packaging, torch\nRequired-by: \n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"!pip install mamba-ssm --no-build-isolation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:10:41.004336Z","iopub.execute_input":"2025-06-22T14:10:41.004650Z","iopub.status.idle":"2025-06-22T14:10:44.257340Z","shell.execute_reply.started":"2025-06-22T14:10:41.004593Z","shell.execute_reply":"2025-06-22T14:10:44.256542Z"},"_kg_hide-input":false},"outputs":[{"name":"stdout","text":"Requirement already satisfied: mamba-ssm in /usr/local/lib/python3.11/dist-packages (2.2.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (2.6.0+cu124)\nRequirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (1.11.1.4)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (0.8.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (4.51.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (25.0)\nRequirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.11/dist-packages (from mamba-ssm) (75.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->mamba-ssm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->mamba-ssm) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba-ssm) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers->mamba-ssm) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers->mamba-ssm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers->mamba-ssm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers->mamba-ssm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers->mamba-ssm) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers->mamba-ssm) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers->mamba-ssm) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba-ssm) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers->mamba-ssm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers->mamba-ssm) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers->mamba-ssm) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers->mamba-ssm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers->mamba-ssm) (2024.2.0)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"!git pull","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T14:57:03.941145Z","iopub.execute_input":"2025-06-22T14:57:03.941891Z","iopub.status.idle":"2025-06-22T14:57:04.574541Z","shell.execute_reply.started":"2025-06-22T14:57:03.941860Z","shell.execute_reply":"2025-06-22T14:57:04.573573Z"}},"outputs":[{"name":"stdout","text":"remote: Enumerating objects: 7, done.\u001b[K\nremote: Counting objects: 100% (7/7), done.\u001b[K\nremote: Compressing objects: 100% (2/2), done.\u001b[K\nremote: Total 4 (delta 2), reused 4 (delta 2), pack-reused 0 (from 0)\u001b[K\nUnpacking objects: 100% (4/4), 560 bytes | 560.00 KiB/s, done.\nFrom https://github.com/sakanaowo/PlantXMamba\n   ec35d99..8e76479  main       -> origin/main\nUpdating ec35d99..8e76479\nFast-forward\n mamba_ssm/__init__.py | 12 \u001b[32m+++++++++++\u001b[m\u001b[31m-\u001b[m\n 1 file changed, 11 insertions(+), 1 deletion(-)\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision.models import VGG16_Weights\nfrom mamba_ssm import Mamba\nimport os\nimport sys\n\n\nclass InceptionBlock(nn.Module):\n    def __init__(self, in_channels=128):\n        super(InceptionBlock, self).__init__()\n        self.branch1x1 = nn.Sequential(\n            nn.Conv2d(in_channels, 128, kernel_size=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(128)\n        )\n        self.branch3x3 = nn.Sequential(\n            nn.Conv2d(in_channels, 96, kernel_size=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(96),\n            nn.Conv2d(96, 128, kernel_size=(3, 1), padding=(1, 0)),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, 128, kernel_size=(1, 3), padding=(0, 1)),\n            nn.ReLU(),\n            nn.BatchNorm2d(128)\n        )\n        self.branch5x5 = nn.Sequential(\n            nn.Conv2d(in_channels, 64, kernel_size=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 96, kernel_size=(3, 1), padding=(1, 0)),\n            nn.ReLU(),\n            nn.BatchNorm2d(96),\n            nn.Conv2d(96, 96, kernel_size=(1, 3), padding=(0, 1)),\n            nn.ReLU(),\n            nn.BatchNorm2d(96),\n            nn.Conv2d(96, 192, kernel_size=(3, 1), padding=(1, 0)),\n            nn.ReLU(),\n            nn.BatchNorm2d(192),\n            nn.Conv2d(192, 192, kernel_size=(1, 3), padding=(0, 1)),\n            nn.ReLU(),\n            nn.BatchNorm2d(192)\n        )\n        self.branch_pool = nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(in_channels, 64, kernel_size=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(64)\n        )\n\n    def forward(self, x):\n        b1 = self.branch1x1(x)\n        b2 = self.branch3x3(x)\n        b3 = self.branch5x5(x)\n        b4 = self.branch_pool(x)\n        return torch.cat([b1, b2, b3, b4], dim=1)\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, in_channels, patch_size=5, emb_size=16):\n        super().__init__()\n        self.patch_size = patch_size\n        self.emb_size = emb_size\n        self.proj = nn.Linear(in_channels * patch_size * patch_size, emb_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n        x = x.view(B, -1, C * self.patch_size * self.patch_size)\n        return self.proj(x)\n\nclass MambaBlock(nn.Module):\n    def __init__(self, emb_size=16, d_state=64, d_conv=4, expand=4):\n        super().__init__()\n        self.norm = nn.LayerNorm(emb_size)\n        self.mamba = Mamba(\n            d_model=emb_size,\n            d_state=d_state,\n            d_conv=d_conv,\n            expand=expand\n        )\n        print(\"Mamba instance:\", self.mamba)\n        self.mlp = nn.Sequential(\n            nn.Linear(emb_size, emb_size * 2),\n            nn.GELU(),\n            nn.Linear(emb_size * 2, emb_size)\n        )\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        residual = x\n        x = self.norm(x)\n        print(\"Input to Mamba:\", x.shape)\n        print(\"Mamba callable:\", callable(self.mamba))\n        x = self.mamba(x)\n        x = residual + x\n        x = x + self.mlp(self.norm(x))\n        return self.dropout(x)\n\nclass PlantXMamba(nn.Module):\n    def __init__(self, num_classes=4, patch_size=5, emb_size=16, num_blocks=4, dropout=0.1):\n        super().__init__()\n        vgg = models.vgg16(weights=VGG16_Weights.DEFAULT)\n        self.vgg_block = nn.Sequential(*list(vgg.features[:10]))\n        self.inception = InceptionBlock(in_channels=128)\n        self.patch_embed = PatchEmbedding(in_channels=512, patch_size=patch_size, emb_size=emb_size)\n        self.transformer = nn.Sequential(*[MambaBlock(emb_size=emb_size, d_state=64, d_conv=4, expand=4) for _ in range(num_blocks)])\n        self.norm = nn.LayerNorm(emb_size)\n        self.global_pool = nn.AdaptiveAvgPool1d(1)\n        self.classifier = nn.Linear(emb_size, num_classes)\n\n    def forward(self, x):\n        x = self.vgg_block(x)\n        x = self.inception(x)\n        x = self.patch_embed(x)\n        x = self.transformer(x)\n        x = self.norm(x)\n        x = x.permute(0, 2, 1)\n        x = self.global_pool(x).squeeze(-1)\n        return self.classifier(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:17:37.308386Z","iopub.execute_input":"2025-06-22T15:17:37.309163Z","iopub.status.idle":"2025-06-22T15:17:37.324984Z","shell.execute_reply.started":"2025-06-22T15:17:37.309136Z","shell.execute_reply":"2025-06-22T15:17:37.324238Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"def test_plantxmamba(batch_size, device, input_size=(3, 224, 224), num_classes=4, patch_size=5, emb_size=16):\n    print(f\"Testing PlantXMamba with batch_size={batch_size} on device={device}\")\n    \n    model = PlantXMamba(\n        num_classes=num_classes,\n        patch_size=patch_size,\n        emb_size=emb_size,\n        num_blocks=4,\n        dropout=0.1\n    ).to(device)\n    \n    model_device = next(model.parameters()).device\n    print(f\"Model is on device: {model_device}\")\n    \n    x = torch.randn(batch_size, *input_size).to(device)\n    print(f\"Input shape: {x.shape}, Input device: {x.device}\")\n    \n    output = None\n    try:\n        model.eval()\n        with torch.no_grad():\n            output = model(x)\n        print(f\"Output shape: {output.shape}\")\n        print(f\"Output device: {output.device}\")\n        \n        expected_output_shape = (batch_size, num_classes)\n        assert output.shape == expected_output_shape, \\\n            f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n        print(\"Test passed successfully!\")\n        \n    except Exception as e:\n        print(f\"Error during testing: {str(e)}\")\n    \n    return output\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    print(\"\\n--- Test Case 1: Batch size = 1 ---\")\n    output1 = test_plantxmamba(batch_size=1, device=device, patch_size=5, emb_size=16)\n    \n    print(\"\\n--- Test Case 2: Batch size = 4 ---\")\n    output2 = test_plantxmamba(batch_size=4, device=device, patch_size=5, emb_size=16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T15:17:41.379289Z","iopub.execute_input":"2025-06-22T15:17:41.379567Z","iopub.status.idle":"2025-06-22T15:17:44.573811Z","shell.execute_reply.started":"2025-06-22T15:17:41.379546Z","shell.execute_reply":"2025-06-22T15:17:44.572986Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\n--- Test Case 1: Batch size = 1 ---\nTesting PlantXMamba with batch_size=1 on device=cuda\nMamba instance: Mamba(\n  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n  (act): SiLU()\n  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n)\nMamba instance: Mamba(\n  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n  (act): SiLU()\n  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n)\nMamba instance: Mamba(\n  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n  (act): SiLU()\n  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n)\nMamba instance: Mamba(\n  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n  (act): SiLU()\n  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n)\nModel is on device: cuda:0\nInput shape: torch.Size([1, 3, 224, 224]), Input device: cuda:0\nInput to Mamba: torch.Size([1, 121, 16])\nMamba callable: True\nError during testing: causal_conv1d_cuda is not available. Please install causal-conv1d.\n\n--- Test Case 2: Batch size = 4 ---\nTesting PlantXMamba with batch_size=4 on device=cuda\nMamba instance: Mamba(\n  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n  (act): SiLU()\n  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n)\nMamba instance: Mamba(\n  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n  (act): SiLU()\n  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n)\nMamba instance: Mamba(\n  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n  (act): SiLU()\n  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n)\nMamba instance: Mamba(\n  (in_proj): Linear(in_features=16, out_features=128, bias=False)\n  (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n  (act): SiLU()\n  (x_proj): Linear(in_features=64, out_features=129, bias=False)\n  (dt_proj): Linear(in_features=1, out_features=64, bias=True)\n  (out_proj): Linear(in_features=64, out_features=16, bias=False)\n)\nModel is on device: cuda:0\nInput shape: torch.Size([4, 3, 224, 224]), Input device: cuda:0\nInput to Mamba: torch.Size([4, 121, 16])\nMamba callable: True\nError during testing: causal_conv1d_cuda is not available. Please install causal-conv1d.\n","output_type":"stream"}],"execution_count":78}]}