{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# PlantXMamba/mamba_block/pscan.py\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def npo2(len):\n",
        "    \"\"\"\n",
        "    Returns the next power of 2 above len\n",
        "    \"\"\"\n",
        "\n",
        "    return 2 ** math.ceil(math.log2(len))\n",
        "\n",
        "\n",
        "def pad_npo2(X):\n",
        "    \"\"\"\n",
        "    Pads input length dim to the next power of 2\n",
        "\n",
        "    Args:\n",
        "        X : (B, L, D, N)\n",
        "\n",
        "    Returns:\n",
        "        Y : (B, npo2(L), D, N)\n",
        "    \"\"\"\n",
        "\n",
        "    len_npo2 = npo2(X.size(1))\n",
        "    pad_tuple = (0, 0, 0, 0, 0, len_npo2 - X.size(1))\n",
        "    return F.pad(X, pad_tuple, \"constant\", 0)\n",
        "\n",
        "\n",
        "class PScan(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def pscan(A, X):\n",
        "        # A : (B, D, L, N)\n",
        "        # X : (B, D, L, N)\n",
        "\n",
        "        # modifies X in place by doing a parallel scan.\n",
        "        # more formally, X will be populated by these values :\n",
        "        # H[t] = A[t] * H[t-1] + X[t] with H[0] = 0\n",
        "        # which are computed in parallel (2*log2(T) sequential steps (ideally), instead of T sequential steps)\n",
        "\n",
        "        # only supports L that is a power of two (mainly for a clearer code)\n",
        "\n",
        "        B, D, L, _ = A.size()\n",
        "        num_steps = int(math.log2(L))\n",
        "\n",
        "        # up sweep (last 2 steps unfolded)\n",
        "        Aa = A\n",
        "        Xa = X\n",
        "        for _ in range(num_steps - 2):\n",
        "            T = Xa.size(2)\n",
        "            Aa = Aa.view(B, D, T // 2, 2, -1)\n",
        "            Xa = Xa.view(B, D, T // 2, 2, -1)\n",
        "\n",
        "            Xa[:, :, :, 1].add_(Aa[:, :, :, 1].mul(Xa[:, :, :, 0]))\n",
        "            Aa[:, :, :, 1].mul_(Aa[:, :, :, 0])\n",
        "\n",
        "            Aa = Aa[:, :, :, 1]\n",
        "            Xa = Xa[:, :, :, 1]\n",
        "\n",
        "        # we have only 4, 2 or 1 nodes left\n",
        "        if Xa.size(2) == 4:\n",
        "            Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 0]))\n",
        "            Aa[:, :, 1].mul_(Aa[:, :, 0])\n",
        "\n",
        "            Xa[:, :, 3].add_(\n",
        "                Aa[:, :, 3].mul(Xa[:, :, 2] + Aa[:, :, 2].mul(Xa[:, :, 1]))\n",
        "            )\n",
        "        elif Xa.size(2) == 2:\n",
        "            Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 0]))\n",
        "            return\n",
        "        else:\n",
        "            return\n",
        "\n",
        "        # down sweep (first 2 steps unfolded)\n",
        "        Aa = A[:, :, 2 ** (num_steps - 2) - 1 : L : 2 ** (num_steps - 2)]\n",
        "        Xa = X[:, :, 2 ** (num_steps - 2) - 1 : L : 2 ** (num_steps - 2)]\n",
        "        Xa[:, :, 2].add_(Aa[:, :, 2].mul(Xa[:, :, 1]))\n",
        "        Aa[:, :, 2].mul_(Aa[:, :, 1])\n",
        "\n",
        "        for k in range(num_steps - 3, -1, -1):\n",
        "            Aa = A[:, :, 2**k - 1 : L : 2**k]\n",
        "            Xa = X[:, :, 2**k - 1 : L : 2**k]\n",
        "\n",
        "            T = Xa.size(2)\n",
        "            Aa = Aa.view(B, D, T // 2, 2, -1)\n",
        "            Xa = Xa.view(B, D, T // 2, 2, -1)\n",
        "\n",
        "            Xa[:, :, 1:, 0].add_(Aa[:, :, 1:, 0].mul(Xa[:, :, :-1, 1]))\n",
        "            Aa[:, :, 1:, 0].mul_(Aa[:, :, :-1, 1])\n",
        "\n",
        "    @staticmethod\n",
        "    def pscan_rev(A, X):\n",
        "        # A : (B, D, L, N)\n",
        "        # X : (B, D, L, N)\n",
        "\n",
        "        # the same function as above, but in reverse\n",
        "        # (if you flip the input, call pscan, then flip the output, you get what this function outputs)\n",
        "        # it is used in the backward pass\n",
        "\n",
        "        # only supports L that is a power of two (mainly for a clearer code)\n",
        "\n",
        "        B, D, L, _ = A.size()\n",
        "        num_steps = int(math.log2(L))\n",
        "\n",
        "        # up sweep (last 2 steps unfolded)\n",
        "        Aa = A\n",
        "        Xa = X\n",
        "        for _ in range(num_steps - 2):\n",
        "            T = Xa.size(2)\n",
        "            Aa = Aa.view(B, D, T // 2, 2, -1)\n",
        "            Xa = Xa.view(B, D, T // 2, 2, -1)\n",
        "\n",
        "            Xa[:, :, :, 0].add_(Aa[:, :, :, 0].mul(Xa[:, :, :, 1]))\n",
        "            Aa[:, :, :, 0].mul_(Aa[:, :, :, 1])\n",
        "\n",
        "            Aa = Aa[:, :, :, 0]\n",
        "            Xa = Xa[:, :, :, 0]\n",
        "\n",
        "        # we have only 4, 2 or 1 nodes left\n",
        "        if Xa.size(2) == 4:\n",
        "            Xa[:, :, 2].add_(Aa[:, :, 2].mul(Xa[:, :, 3]))\n",
        "            Aa[:, :, 2].mul_(Aa[:, :, 3])\n",
        "\n",
        "            Xa[:, :, 0].add_(\n",
        "                Aa[:, :, 0].mul(Xa[:, :, 1].add(Aa[:, :, 1].mul(Xa[:, :, 2])))\n",
        "            )\n",
        "        elif Xa.size(2) == 2:\n",
        "            Xa[:, :, 0].add_(Aa[:, :, 0].mul(Xa[:, :, 1]))\n",
        "            return\n",
        "        else:\n",
        "            return\n",
        "\n",
        "        # down sweep (first 2 steps unfolded)\n",
        "        Aa = A[:, :, 0 : L : 2 ** (num_steps - 2)]\n",
        "        Xa = X[:, :, 0 : L : 2 ** (num_steps - 2)]\n",
        "        Xa[:, :, 1].add_(Aa[:, :, 1].mul(Xa[:, :, 2]))\n",
        "        Aa[:, :, 1].mul_(Aa[:, :, 2])\n",
        "\n",
        "        for k in range(num_steps - 3, -1, -1):\n",
        "            Aa = A[:, :, 0 : L : 2**k]\n",
        "            Xa = X[:, :, 0 : L : 2**k]\n",
        "\n",
        "            T = Xa.size(2)\n",
        "            Aa = Aa.view(B, D, T // 2, 2, -1)\n",
        "            Xa = Xa.view(B, D, T // 2, 2, -1)\n",
        "\n",
        "            Xa[:, :, :-1, 1].add_(Aa[:, :, :-1, 1].mul(Xa[:, :, 1:, 0]))\n",
        "            Aa[:, :, :-1, 1].mul_(Aa[:, :, 1:, 0])\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, A_in, X_in):\n",
        "        \"\"\"\n",
        "        Applies the parallel scan operation, as defined above. Returns a new tensor.\n",
        "        If you can, privilege sequence lengths that are powers of two.\n",
        "\n",
        "        Args:\n",
        "            A_in : (B, L, D, N)\n",
        "            X_in : (B, L, D, N)\n",
        "\n",
        "        Returns:\n",
        "            H : (B, L, D, N)\n",
        "        \"\"\"\n",
        "\n",
        "        L = X_in.size(1)\n",
        "\n",
        "        # cloning is requiered because of the in-place ops\n",
        "        if L == npo2(L):\n",
        "            A = A_in.clone()\n",
        "            X = X_in.clone()\n",
        "        else:\n",
        "            # pad tensors (and clone btw)\n",
        "            A = pad_npo2(A_in)  # (B, npo2(L), D, N)\n",
        "            X = pad_npo2(X_in)  # (B, npo2(L), D, N)\n",
        "\n",
        "        # prepare tensors\n",
        "        A = A.transpose(2, 1)  # (B, D, npo2(L), N)\n",
        "        X = X.transpose(2, 1)  # (B, D, npo2(L), N)\n",
        "\n",
        "        # parallel scan (modifies X in-place)\n",
        "        PScan.pscan(A, X)\n",
        "\n",
        "        ctx.save_for_backward(A_in, X)\n",
        "\n",
        "        # slice [:, :L] (cut if there was padding)\n",
        "        return X.transpose(2, 1)[:, :L]\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output_in):\n",
        "        \"\"\"\n",
        "        Flows the gradient from the output to the input. Returns two new tensors.\n",
        "\n",
        "        Args:\n",
        "            ctx : A_in : (B, L, D, N), X : (B, D, L, N)\n",
        "            grad_output_in : (B, L, D, N)\n",
        "\n",
        "        Returns:\n",
        "            gradA : (B, L, D, N), gradX : (B, L, D, N)\n",
        "        \"\"\"\n",
        "\n",
        "        A_in, X = ctx.saved_tensors\n",
        "\n",
        "        L = grad_output_in.size(1)\n",
        "\n",
        "        # cloning is requiered because of the in-place ops\n",
        "        if L == npo2(L):\n",
        "            grad_output = grad_output_in.clone()\n",
        "            # the next padding will clone A_in\n",
        "        else:\n",
        "            grad_output = pad_npo2(grad_output_in)  # (B, npo2(L), D, N)\n",
        "            A_in = pad_npo2(A_in)  # (B, npo2(L), D, N)\n",
        "\n",
        "        # prepare tensors\n",
        "        grad_output = grad_output.transpose(2, 1)\n",
        "        A_in = A_in.transpose(2, 1)  # (B, D, npo2(L), N)\n",
        "        A = torch.nn.functional.pad(\n",
        "            A_in[:, :, 1:], (0, 0, 0, 1)\n",
        "        )  # (B, D, npo2(L), N) shift 1 to the left (see hand derivation)\n",
        "\n",
        "        # reverse parallel scan (modifies grad_output in-place)\n",
        "        PScan.pscan_rev(A, grad_output)\n",
        "\n",
        "        Q = torch.zeros_like(X)\n",
        "        Q[:, :, 1:].add_(X[:, :, :-1] * grad_output[:, :, 1:])\n",
        "\n",
        "        return Q.transpose(2, 1)[:, :L], grad_output.transpose(2, 1)[:, :L]\n",
        "\n",
        "\n",
        "pscan = PScan.apply\n"
      ],
      "metadata": {
        "id": "lXbEDCrRVVIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PlantXMamba/mamba_block/backbone.py\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Union\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "This file closely follows the mamba_simple.py from the official Mamba implementation, and the mamba-minimal by @johnma2006.\n",
        "The major differences are :\n",
        "-the convolution is done with torch.nn.Conv1d\n",
        "-the selective scan is done in PyTorch\n",
        "\n",
        "A sequential version of the selective scan is also available for comparison. Also, it is possible to use the official Mamba implementation.\n",
        "\n",
        "This is the structure of the torch modules :\n",
        "- A Mamba model is composed of several layers, which are ResidualBlock.\n",
        "- A ResidualBlock is composed of a MambaBlock, a normalization, and a residual connection : ResidualBlock(x) = mamba(norm(x)) + x\n",
        "- This leaves us with the MambaBlock : its input x is (B, L, D) and its outputs y is also (B, L, D) (B=batch size, L=seq len, D=model dim).\n",
        "First, we expand x into (B, L, 2*ED) (where E is usually 2) and split it into x and z, each (B, L, ED).\n",
        "Then, we apply the short 1d conv to x, followed by an activation function (silu), then the SSM.\n",
        "We then multiply it by silu(z).\n",
        "See Figure 3 of the paper (page 8) for a visual representation of a MambaBlock.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MambaConfig:\n",
        "    d_model: int  # D\n",
        "    n_layers: int\n",
        "    dt_rank: Union[int, str] = \"auto\"\n",
        "    d_state: int = 16  # N in paper/comments\n",
        "    expand_factor: int = 2  # E in paper/comments\n",
        "    d_conv: int = 4\n",
        "\n",
        "    dt_min: float = 0.001\n",
        "    dt_max: float = 0.1\n",
        "    dt_init: str = \"random\"  # \"random\" or \"constant\"\n",
        "    dt_scale: float = 1.0\n",
        "    dt_init_floor = 1e-4\n",
        "\n",
        "    rms_norm_eps: float = 1e-5\n",
        "    base_std: float = 0.02\n",
        "\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    bias: bool = False\n",
        "    conv_bias: bool = True\n",
        "    inner_layernorms: bool = False  # apply layernorms to internal activations\n",
        "\n",
        "    mup: bool = False\n",
        "    mup_base_width: float = 128  # width=d_model\n",
        "\n",
        "    pscan: bool = True  # use parallel scan mode or sequential mode when training\n",
        "    use_cuda: bool = False  # use official CUDA implementation when training (not compatible with (b)float16)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.d_inner = self.expand_factor * self.d_model  # E*D = ED in comments\n",
        "\n",
        "        if self.dt_rank == \"auto\":\n",
        "            self.dt_rank = math.ceil(self.d_model / 16)\n",
        "\n",
        "        # muP\n",
        "        if self.mup:\n",
        "            self.mup_width_mult = self.d_model / self.mup_base_width\n",
        "\n",
        "\n",
        "class Mamba(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [ResidualBlock(config) for _ in range(config.n_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x : (B, L, D)\n",
        "\n",
        "        # y : (B, L, D)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def step(self, x, caches):\n",
        "        # x : (B, L, D)\n",
        "        # caches : [cache(layer) for all layers], cache : (h, inputs)\n",
        "\n",
        "        # y : (B, L, D)\n",
        "        # caches : [cache(layer) for all layers], cache : (h, inputs)\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x, caches[i] = layer.step(x, caches[i])\n",
        "\n",
        "        return x, caches\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mixer = MambaBlock(config)\n",
        "        self.norm = RMSNorm(config.d_model, config.rms_norm_eps, config.mup)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x : (B, L, D)\n",
        "\n",
        "        # output : (B, L, D)\n",
        "\n",
        "        output = self.mixer(self.norm(x)) + x\n",
        "        return output\n",
        "\n",
        "    def step(self, x, cache):\n",
        "        # x : (B, D)\n",
        "        # cache : (h, inputs)\n",
        "        # h : (B, ED, N)\n",
        "        # inputs: (B, ED, d_conv-1)\n",
        "\n",
        "        # output : (B, D)\n",
        "        # cache : (h, inputs)\n",
        "\n",
        "        output, cache = self.mixer.step(self.norm(x), cache)\n",
        "        output = output + x\n",
        "        return output, cache\n",
        "\n",
        "\n",
        "class MambaBlock(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        # projects block input from D to 2*ED (two branches)\n",
        "        self.in_proj = nn.Linear(config.d_model, 2 * config.d_inner, bias=config.bias)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            in_channels=config.d_inner,\n",
        "            out_channels=config.d_inner,\n",
        "            kernel_size=config.d_conv,\n",
        "            bias=config.conv_bias,\n",
        "            groups=config.d_inner,\n",
        "            padding=config.d_conv - 1,\n",
        "        )\n",
        "\n",
        "        # projects x to input-dependent delta, B, C\n",
        "        self.x_proj = nn.Linear(\n",
        "            config.d_inner, config.dt_rank + 2 * config.d_state, bias=False\n",
        "        )\n",
        "\n",
        "        # projects delta from dt_rank to d_inner\n",
        "        self.dt_proj = nn.Linear(config.dt_rank, config.d_inner, bias=True)\n",
        "\n",
        "        # dt initialization\n",
        "        # dt weights\n",
        "        dt_init_std = config.dt_rank**-0.5 * config.dt_scale\n",
        "        if config.dt_init == \"constant\":\n",
        "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
        "        elif config.dt_init == \"random\":\n",
        "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # delta bias\n",
        "        dt = torch.exp(\n",
        "            torch.rand(config.d_inner)\n",
        "            * (math.log(config.dt_max) - math.log(config.dt_min))\n",
        "            + math.log(config.dt_min)\n",
        "        ).clamp(min=config.dt_init_floor)\n",
        "        inv_dt = dt + torch.log(\n",
        "            -torch.expm1(-dt)\n",
        "        )  # inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
        "        with torch.no_grad():\n",
        "            self.dt_proj.bias.copy_(inv_dt)\n",
        "        # self.dt_proj.bias._no_reinit = True # initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
        "        # todo : explain why removed\n",
        "\n",
        "        # S4D real initialization\n",
        "        A = torch.arange(1, config.d_state + 1, dtype=torch.float32).repeat(\n",
        "            config.d_inner, 1\n",
        "        )\n",
        "        self.A_log = nn.Parameter(\n",
        "            torch.log(A)\n",
        "        )  # why store A in log ? to keep A < 0 (cf -torch.exp(...)) ? for gradient stability ?\n",
        "        self.A_log._no_weight_decay = True\n",
        "\n",
        "        self.D = nn.Parameter(torch.ones(config.d_inner))\n",
        "        self.D._no_weight_decay = True\n",
        "\n",
        "        # projects block output from ED back to D\n",
        "        self.out_proj = nn.Linear(config.d_inner, config.d_model, bias=config.bias)\n",
        "\n",
        "        # used in jamba\n",
        "        if self.config.inner_layernorms:\n",
        "            self.dt_layernorm = RMSNorm(\n",
        "                self.config.dt_rank, config.rms_norm_eps, config.mup\n",
        "            )\n",
        "            self.B_layernorm = RMSNorm(\n",
        "                self.config.d_state, config.rms_norm_eps, config.mup\n",
        "            )\n",
        "            self.C_layernorm = RMSNorm(\n",
        "                self.config.d_state, config.rms_norm_eps, config.mup\n",
        "            )\n",
        "        else:\n",
        "            self.dt_layernorm = None\n",
        "            self.B_layernorm = None\n",
        "            self.C_layernorm = None\n",
        "\n",
        "        if self.config.use_cuda:\n",
        "            try:\n",
        "                from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n",
        "\n",
        "                self.selective_scan_cuda = selective_scan_fn\n",
        "            except ImportError:\n",
        "                print(\"Failed to import mamba_ssm. Falling back to mamba.py.\")\n",
        "                self.config.use_cuda = False\n",
        "\n",
        "    def _apply_layernorms(self, dt, B, C):\n",
        "        if self.dt_layernorm is not None:\n",
        "            dt = self.dt_layernorm(dt)\n",
        "        if self.B_layernorm is not None:\n",
        "            B = self.B_layernorm(B)\n",
        "        if self.C_layernorm is not None:\n",
        "            C = self.C_layernorm(C)\n",
        "        return dt, B, C\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x : (B, L, D)\n",
        "\n",
        "        # y : (B, L, D)\n",
        "\n",
        "        _, L, _ = x.shape\n",
        "\n",
        "        xz = self.in_proj(x)  # (B, L, 2*ED)\n",
        "        x, z = xz.chunk(2, dim=-1)  # (B, L, ED), (B, L, ED)\n",
        "\n",
        "        # x branch\n",
        "        x = x.transpose(1, 2)  # (B, ED, L)\n",
        "        x = self.conv1d(x)[\n",
        "            :, :, :L\n",
        "        ]  # depthwise convolution over time, with a short filter\n",
        "        x = x.transpose(1, 2)  # (B, L, ED)\n",
        "\n",
        "        x = F.silu(x)\n",
        "        y = self.ssm(x, z)\n",
        "\n",
        "        if self.config.use_cuda:\n",
        "            output = self.out_proj(y)  # (B, L, D)\n",
        "            return output  # the rest of the operations are done in the ssm function (fused with the CUDA pscan)\n",
        "\n",
        "        # z branch\n",
        "        z = F.silu(z)\n",
        "\n",
        "        output = y * z\n",
        "        output = self.out_proj(output)  # (B, L, D)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def ssm(self, x, z):\n",
        "        # x : (B, L, ED)\n",
        "\n",
        "        # y : (B, L, ED)\n",
        "\n",
        "        A = -torch.exp(self.A_log.float())  # (ED, N)\n",
        "        D = self.D.float()\n",
        "\n",
        "        deltaBC = self.x_proj(x)  # (B, L, dt_rank+2*N)\n",
        "        delta, B, C = torch.split(\n",
        "            deltaBC,\n",
        "            [self.config.dt_rank, self.config.d_state, self.config.d_state],\n",
        "            dim=-1,\n",
        "        )  # (B, L, dt_rank), (B, L, N), (B, L, N)\n",
        "        delta, B, C = self._apply_layernorms(delta, B, C)\n",
        "        delta = self.dt_proj.weight @ delta.transpose(\n",
        "            1, 2\n",
        "        )  # (ED, dt_rank) @ (B, L, dt_rank) -> (B, ED, L)\n",
        "        # here we just apply the matrix mul operation of delta = softplus(dt_proj(delta))\n",
        "        # the rest will be applied later (fused if using cuda)\n",
        "\n",
        "        # choose which selective_scan function to use, according to config\n",
        "        if self.config.use_cuda:\n",
        "            # these are unfortunately needed for the selective_scan_cuda function\n",
        "            x = x.transpose(1, 2)\n",
        "            B = B.transpose(1, 2)\n",
        "            C = C.transpose(1, 2)\n",
        "            z = z.transpose(1, 2)\n",
        "\n",
        "            # \"softplus\" + \"bias\" + \"y * silu(z)\" operations are fused\n",
        "            y = self.selective_scan_cuda(\n",
        "                x,\n",
        "                delta,\n",
        "                A,\n",
        "                B,\n",
        "                C,\n",
        "                D,\n",
        "                z=z,\n",
        "                delta_softplus=True,\n",
        "                delta_bias=self.dt_proj.bias.float(),\n",
        "            )\n",
        "            y = y.transpose(1, 2)  # (B, L, ED)\n",
        "\n",
        "        else:\n",
        "            delta = delta.transpose(1, 2)\n",
        "            delta = F.softplus(delta + self.dt_proj.bias)\n",
        "\n",
        "            if self.config.pscan:\n",
        "                y = self.selective_scan(x, delta, A, B, C, D)\n",
        "            else:\n",
        "                y = self.selective_scan_seq(x, delta, A, B, C, D)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def selective_scan(self, x, delta, A, B, C, D):\n",
        "        # x : (B, L, ED)\n",
        "        # Δ : (B, L, ED)\n",
        "        # A : (ED, N)\n",
        "        # B : (B, L, N)\n",
        "        # C : (B, L, N)\n",
        "        # D : (ED)\n",
        "\n",
        "        # y : (B, L, ED)\n",
        "\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, L, ED, N)\n",
        "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  # (B, L, ED, N)\n",
        "\n",
        "        BX = deltaB * (x.unsqueeze(-1))  # (B, L, ED, N)\n",
        "\n",
        "        hs = pscan(deltaA, BX)\n",
        "\n",
        "        y = (hs @ C.unsqueeze(-1)).squeeze(\n",
        "            3\n",
        "        )  # (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n",
        "\n",
        "        y = y + D * x\n",
        "\n",
        "        return y\n",
        "\n",
        "    def selective_scan_seq(self, x, delta, A, B, C, D):\n",
        "        # x : (B, L, ED)\n",
        "        # Δ : (B, L, ED)\n",
        "        # A : (ED, N)\n",
        "        # B : (B, L, N)\n",
        "        # C : (B, L, N)\n",
        "        # D : (ED)\n",
        "\n",
        "        # y : (B, L, ED)\n",
        "\n",
        "        _, L, _ = x.shape\n",
        "\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, L, ED, N)\n",
        "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  # (B, L, ED, N)\n",
        "\n",
        "        BX = deltaB * (x.unsqueeze(-1))  # (B, L, ED, N)\n",
        "\n",
        "        h = torch.zeros(\n",
        "            x.size(0), self.config.d_inner, self.config.d_state, device=deltaA.device\n",
        "        )  # (B, ED, N)\n",
        "        hs = []\n",
        "\n",
        "        for t in range(0, L):\n",
        "            h = deltaA[:, t] * h + BX[:, t]\n",
        "            hs.append(h)\n",
        "\n",
        "        hs = torch.stack(hs, dim=1)  # (B, L, ED, N)\n",
        "\n",
        "        y = (hs @ C.unsqueeze(-1)).squeeze(\n",
        "            3\n",
        "        )  # (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n",
        "\n",
        "        y = y + D * x\n",
        "\n",
        "        return y\n",
        "\n",
        "    # -------------------------- inference -------------------------- #\n",
        "    \"\"\"\n",
        "    Concerning auto-regressive inference\n",
        "\n",
        "    The cool part of using Mamba : inference is constant wrt to sequence length\n",
        "    We just have to keep in cache, for each layer, two things :\n",
        "    - the hidden state h (which is (B, ED, N)), as you typically would when doing inference with a RNN\n",
        "    - the last d_conv-1 inputs of the layer, to be able to compute the 1D conv which is a convolution over the time dimension\n",
        "      (d_conv is fixed so this doesn't incur a growing cache as we progress on generating the sequence)\n",
        "      (and d_conv is usually very small, like 4, so we just have to \"remember\" the last 3 inputs)\n",
        "\n",
        "    Concretely, these two quantities are put inside a cache tuple, and are named h and inputs respectively.\n",
        "    h is (B, ED, N), and inputs is (B, ED, d_conv-1)\n",
        "    The MambaBlock.step() receives this cache, and, along with outputing the output, alos outputs the updated cache for the next call.\n",
        "\n",
        "    The cache object is initialized as follows : (None, torch.zeros()).\n",
        "    When h is None, the selective scan function detects it and start with h=0.\n",
        "    The torch.zeros() isn't a problem (it's same as just feeding the input, because the conv1d is padded)\n",
        "\n",
        "    As we need one such cache variable per layer, we store a caches object, which is simply a list of cache object. (See mamba_lm.py)\n",
        "    \"\"\"\n",
        "\n",
        "    def step(self, x, cache):\n",
        "        # x : (B, D)\n",
        "        # cache : (h, inputs)\n",
        "        # h : (B, ED, N)\n",
        "        # inputs : (B, ED, d_conv-1)\n",
        "\n",
        "        # y : (B, D)\n",
        "        # cache : (h, inputs)\n",
        "\n",
        "        h, inputs = cache\n",
        "\n",
        "        xz = self.in_proj(x)  # (B, 2*ED)\n",
        "        x, z = xz.chunk(2, dim=1)  # (B, ED), (B, ED)\n",
        "\n",
        "        # x branch\n",
        "        x_cache = x.unsqueeze(2)\n",
        "        x = self.conv1d(torch.cat([inputs, x_cache], dim=2))[\n",
        "            :, :, self.config.d_conv - 1\n",
        "        ]  # (B, ED)\n",
        "\n",
        "        x = F.silu(x)\n",
        "        y, h = self.ssm_step(x, h)\n",
        "\n",
        "        # z branch\n",
        "        z = F.silu(z)\n",
        "\n",
        "        output = y * z\n",
        "        output = self.out_proj(output)  # (B, D)\n",
        "\n",
        "        # prepare cache for next call\n",
        "        inputs = torch.cat([inputs[:, :, 1:], x_cache], dim=2)  # (B, ED, d_conv-1)\n",
        "        cache = (h, inputs)\n",
        "\n",
        "        return output, cache\n",
        "\n",
        "    def ssm_step(self, x, h):\n",
        "        # x : (B, ED)\n",
        "        # h : (B, ED, N)\n",
        "\n",
        "        # y : (B, ED)\n",
        "        # h : (B, ED, N)\n",
        "\n",
        "        A = -torch.exp(\n",
        "            self.A_log.float()\n",
        "        )  # (ED, N) # todo : ne pas le faire tout le temps, puisque c'est indépendant de la timestep\n",
        "        D = self.D.float()\n",
        "\n",
        "        deltaBC = self.x_proj(x)  # (B, dt_rank+2*N)\n",
        "\n",
        "        delta, B, C = torch.split(\n",
        "            deltaBC,\n",
        "            [self.config.dt_rank, self.config.d_state, self.config.d_state],\n",
        "            dim=-1,\n",
        "        )  # (B, dt_rank), (B, N), (B, N)\n",
        "        delta, B, C = self._apply_layernorms(delta, B, C)\n",
        "        delta = F.softplus(self.dt_proj(delta))  # (B, ED)\n",
        "\n",
        "        deltaA = torch.exp(delta.unsqueeze(-1) * A)  # (B, ED, N)\n",
        "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(1)  # (B, ED, N)\n",
        "\n",
        "        BX = deltaB * (x.unsqueeze(-1))  # (B, ED, N)\n",
        "\n",
        "        if h is None:\n",
        "            h = torch.zeros(\n",
        "                x.size(0),\n",
        "                self.config.d_inner,\n",
        "                self.config.d_state,\n",
        "                device=deltaA.device,\n",
        "            )  # (B, ED, N)\n",
        "\n",
        "        h = deltaA * h + BX  # (B, ED, N)\n",
        "\n",
        "        y = (h @ C.unsqueeze(-1)).squeeze(2)  # (B, ED, N) @ (B, N, 1) -> (B, ED, 1)\n",
        "\n",
        "        y = y + D * x\n",
        "\n",
        "        return y, h\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, d_model: int, eps: float = 1e-5, use_mup: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_mup = use_mup\n",
        "        self.eps = eps\n",
        "\n",
        "        # https://arxiv.org/abs/2404.05728, RMSNorm gains prevents muTransfer (section 4.2.3)\n",
        "        if not use_mup:\n",
        "            self.weight = nn.Parameter(torch.ones(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "        if not self.use_mup:\n",
        "            return output * self.weight\n",
        "        else:\n",
        "            return output\n"
      ],
      "metadata": {
        "id": "bSTVFq6PXOo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PlantXMamba/mamba_block/head.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MambaHead(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.norm(x)\n",
        "        x = self.dropout(x)\n",
        "        return x  # (batch_size, seq_len, d_model)\n"
      ],
      "metadata": {
        "id": "dBJZv0xKVoo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PlantXMamba/mamba_block/model.py\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MambaModule(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.d_model = self.args.d_model\n",
        "        self.n_layers = self.args.n_layers\n",
        "\n",
        "        config = MambaConfig(d_model=self.d_model, n_layers=self.n_layers,\n",
        "                           d_state=self.args.d_state, d_conv=self.args.d_conv,\n",
        "                           expand_factor=self.args.expand,dropout=self.args.dropout)\n",
        "        self.backbone = Mamba(config)\n",
        "        self.head = MambaHead(d_model=self.d_model, dropout=self.args.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        sequence_output = self.backbone(x)  # (batch_size, seq_len, d_model)\n",
        "        output = self.head(sequence_output)  # (batch_size, seq_len, d_model)\n",
        "        return output"
      ],
      "metadata": {
        "id": "4g3JLFmvVxlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ-_nnypTG1C",
        "outputId": "650a4278-9ee0-4188-a520-13d02ec90754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PlantXViT'...\n",
            "remote: Enumerating objects: 104825, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 104825 (delta 6), reused 20 (delta 4), pack-reused 104802 (from 1)\u001b[K\n",
            "Receiving objects: 100% (104825/104825), 2.45 GiB | 58.18 MiB/s, done.\n",
            "Resolving deltas: 100% (30447/30447), done.\n",
            "Updating files: 100% (104353/104353), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sakanaowo/PlantXViT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PlantXViT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIkoGfLUUV5e",
        "outputId": "fc0f967c-1e06-48ee-c0c6-623bbdd44417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PlantXViT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.0),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "E8p3VAAyUXmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.models import VGG16_Weights\n",
        "\n",
        "import os\n",
        "root_dir=\"./data/raw/embrapa\""
      ],
      "metadata": {
        "id": "tsLJ956HU-fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.ImageFolder(os.path.join(root_dir, \"train\"), transform=image_transforms)\n",
        "val_dataset = datasets.ImageFolder(os.path.join(root_dir, \"val\"), transform=image_transforms)\n",
        "test_dataset = datasets.ImageFolder(os.path.join(root_dir, \"test\"), transform=image_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "I4kECdRZVAv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels=128):\n",
        "        super(InceptionBlock, self).__init__()\n",
        "        # Nhánh 1: 1x1\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 128, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128)\n",
        "        )\n",
        "\n",
        "        # Nhánh 2: 1x1 -> 3x1 + 1x3\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 96, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.Conv2d(96, 128, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128)\n",
        "        )\n",
        "\n",
        "        # Nhánh 3: 1x1 -> 3x1 + 1x3 -> 3x1 + 1x3\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 96, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.Conv2d(96, 96, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.Conv2d(96, 192, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.Conv2d(192, 192, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(192)\n",
        "        )\n",
        "\n",
        "        # Nhánh 4: MaxPool -> 1x1\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.branch1x1(x)\n",
        "        b2 = self.branch3x3(x)\n",
        "        b3 = self.branch5x5(x)\n",
        "        b4 = self.branch_pool(x)\n",
        "        return torch.cat([b1, b2, b3, b4], dim=1)"
      ],
      "metadata": {
        "id": "iIzSnpzEVBa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size=5, emb_size=16):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.emb_size = emb_size\n",
        "        self.proj = nn.Linear(in_channels * patch_size * patch_size, emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
        "        x = x.view(B, -1, C * self.patch_size * self.patch_size)\n",
        "        return self.proj(x)  # shape: (b,num patches,emb size)"
      ],
      "metadata": {
        "id": "8CNhgD66VF_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlantXMamba(nn.Module):\n",
        "    def __init__(self, num_classes=4, patch_size=5, emb_size=16, d_state=64,d_conv=64,expand=4,n_layers=2,num_blocks=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # VGG16 (2 blocks)\n",
        "        vgg = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
        "        self.vgg_block = nn.Sequential(*list(vgg.features[:10]))\n",
        "\n",
        "        # Inception-like block → (B, 512, 56, 56)\n",
        "        self.inception = InceptionBlock(in_channels=128)\n",
        "\n",
        "        # Patch Embedding → (B, 121, 16)\n",
        "        self.patch_embed = PatchEmbedding(in_channels=512, patch_size=patch_size, emb_size=emb_size)\n",
        "\n",
        "        # Mamba blocks\n",
        "        mamba_args = type('Args', (), {\n",
        "            'd_model': emb_size,\n",
        "            'd_state': d_state,\n",
        "            'd_conv': d_conv,\n",
        "            'expand': expand,\n",
        "            'n_layers': n_layers,\n",
        "            'dropout': dropout\n",
        "        })()\n",
        "        self.mamba = nn.Sequential(*[MambaModule(mamba_args) for _ in range(num_blocks)])\n",
        "\n",
        "        # Classification head\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vgg_block(x)  # (B, 128, 56, 56)\n",
        "        x = self.inception(x)  # (B, 512, 56, 56)\n",
        "        x = self.patch_embed(x)  # (B, 121, 16)\n",
        "        x = self.mamba(x)  # (B, 121, 16)\n",
        "        x = self.norm(x)  # (B, 121, 16)\n",
        "        x = x.permute(0, 2, 1)  # (B, 16, 121)\n",
        "        x = self.global_pool(x).squeeze(-1)  # (B, 16)\n",
        "        return self.classifier(x)  # (B, num_classes)"
      ],
      "metadata": {
        "id": "MWhgugQkV8D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PlantXMamba(num_classes=93)\n",
        "criterion=nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-YeE5l1WEmg",
        "outputId": "0a5a6bd3-29f4-43f1-8e1a-8b676ababf08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:02<00:00, 202MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.config_loader import load_config\n",
        "config=load_config('./configs/config.yaml')\n",
        "print(config['output']['embrapa']['model_path'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvdewSEuWGkg",
        "outputId": "88f21592-1bd9-42a3-8cc2-7c30b04a5c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./outputs/embrapa/models/plantxvit_best.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR=root_dir\n",
        "BATCH_SIZE=16\n",
        "EPOCHS=50\n",
        "LR=1e-4\n",
        "NUM_CLASSES=93\n",
        "DEVICE=torch.device('cuda')\n",
        "MODEL_PATH = \"./outputs/embrapa/models/plantxvit_best.pth\"\n",
        "\n",
        "# Tạo thư mục nếu chưa tồn tại\n",
        "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)"
      ],
      "metadata": {
        "id": "odhAAZ1iWMlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "GZXgdvP3WORy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for inputs, labels in tqdm(loader, desc=\"Training\"):\n",
        "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc"
      ],
      "metadata": {
        "id": "VYF6fiNJWW3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=\"Evaluating\"):\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc"
      ],
      "metadata": {
        "id": "_x9Gp2-0WYdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_acc = 0\n",
        "patience,wait=5,0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        print(f\"✅ Saved best model to {MODEL_PATH}\")\n",
        "        wait=0\n",
        "    else:\n",
        "      wait+=1\n",
        "      if wait>=patience:\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG9AxQ2qWatC",
        "outputId": "02351659-93d1-44f1-ae2b-8685b50977b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:29<00:00, 12.41it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.3367 | Acc: 0.3167\n",
            "Val   Loss: 2.5670 | Acc: 0.4563\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.63it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.2750 | Acc: 0.5058\n",
            "Val   Loss: 1.8892 | Acc: 0.5426\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 3/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.62it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7443 | Acc: 0.5951\n",
            "Val   Loss: 1.4785 | Acc: 0.6376\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 4/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:25<00:00, 12.70it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.4317 | Acc: 0.6518\n",
            "Val   Loss: 1.2624 | Acc: 0.6795\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 5/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:25<00:00, 12.72it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.2295 | Acc: 0.6939\n",
            "Val   Loss: 1.0822 | Acc: 0.7175\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 6/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.61it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0934 | Acc: 0.7191\n",
            "Val   Loss: 0.9924 | Acc: 0.7390\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 7/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:25<00:00, 12.70it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9748 | Acc: 0.7434\n",
            "Val   Loss: 0.9077 | Acc: 0.7553\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 8/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:25<00:00, 12.68it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9015 | Acc: 0.7593\n",
            "Val   Loss: 0.8552 | Acc: 0.7729\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 9/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:25<00:00, 12.69it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8244 | Acc: 0.7769\n",
            "Val   Loss: 0.7661 | Acc: 0.7952\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 10/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:25<00:00, 12.70it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7603 | Acc: 0.7921\n",
            "Val   Loss: 0.7467 | Acc: 0.7928\n",
            "\n",
            "Epoch 11/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.62it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7094 | Acc: 0.8036\n",
            "Val   Loss: 0.6873 | Acc: 0.8133\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 12/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:25<00:00, 12.68it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 29.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6610 | Acc: 0.8164\n",
            "Val   Loss: 0.6930 | Acc: 0.8114\n",
            "\n",
            "Epoch 13/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:25<00:00, 12.70it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6234 | Acc: 0.8276\n",
            "Val   Loss: 0.6464 | Acc: 0.8231\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 14/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.64it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5852 | Acc: 0.8352\n",
            "Val   Loss: 0.5944 | Acc: 0.8317\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 15/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:25<00:00, 12.68it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5529 | Acc: 0.8420\n",
            "Val   Loss: 0.6060 | Acc: 0.8341\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 16/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.65it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5154 | Acc: 0.8535\n",
            "Val   Loss: 0.5776 | Acc: 0.8401\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 17/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.59it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4965 | Acc: 0.8560\n",
            "Val   Loss: 0.5532 | Acc: 0.8437\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 18/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.64it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4754 | Acc: 0.8614\n",
            "Val   Loss: 0.5669 | Acc: 0.8417\n",
            "\n",
            "Epoch 19/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.62it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4471 | Acc: 0.8706\n",
            "Val   Loss: 0.4946 | Acc: 0.8601\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 20/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.65it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4221 | Acc: 0.8777\n",
            "Val   Loss: 0.5320 | Acc: 0.8506\n",
            "\n",
            "Epoch 21/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:27<00:00, 12.56it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3978 | Acc: 0.8843\n",
            "Val   Loss: 0.4912 | Acc: 0.8630\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 22/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.67it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3933 | Acc: 0.8836\n",
            "Val   Loss: 0.4824 | Acc: 0.8656\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 23/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.60it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3681 | Acc: 0.8908\n",
            "Val   Loss: 0.4734 | Acc: 0.8588\n",
            "\n",
            "Epoch 24/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.63it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3496 | Acc: 0.8963\n",
            "Val   Loss: 0.5244 | Acc: 0.8516\n",
            "\n",
            "Epoch 25/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.64it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3330 | Acc: 0.9015\n",
            "Val   Loss: 0.4472 | Acc: 0.8716\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 26/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.64it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 29.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3320 | Acc: 0.8998\n",
            "Val   Loss: 0.4953 | Acc: 0.8597\n",
            "\n",
            "Epoch 27/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.62it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3100 | Acc: 0.9083\n",
            "Val   Loss: 0.4702 | Acc: 0.8661\n",
            "\n",
            "Epoch 28/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.64it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 29.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2934 | Acc: 0.9139\n",
            "Val   Loss: 0.4789 | Acc: 0.8640\n",
            "\n",
            "Epoch 29/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.61it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2906 | Acc: 0.9134\n",
            "Val   Loss: 0.5310 | Acc: 0.8511\n",
            "\n",
            "Epoch 30/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.65it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 29.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2735 | Acc: 0.9196\n",
            "Val   Loss: 0.4331 | Acc: 0.8774\n",
            "✅ Saved best model to ./outputs/embrapa/models/plantxvit_best.pth\n",
            "\n",
            "Epoch 31/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:25<00:00, 12.72it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2591 | Acc: 0.9227\n",
            "Val   Loss: 0.4545 | Acc: 0.8732\n",
            "\n",
            "Epoch 32/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.63it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2596 | Acc: 0.9209\n",
            "Val   Loss: 0.4568 | Acc: 0.8765\n",
            "\n",
            "Epoch 33/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:25<00:00, 12.70it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2435 | Acc: 0.9279\n",
            "Val   Loss: 0.4961 | Acc: 0.8643\n",
            "\n",
            "Epoch 34/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.68it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2367 | Acc: 0.9295\n",
            "Val   Loss: 0.4604 | Acc: 0.8720\n",
            "\n",
            "Epoch 35/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1851/1851 [02:26<00:00, 12.66it/s]\n",
            "Evaluating: 100%|██████████| 466/466 [00:15<00:00, 30.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2218 | Acc: 0.9323\n",
            "Val   Loss: 0.4513 | Acc: 0.8743\n",
            "Early stopping at epoch 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}